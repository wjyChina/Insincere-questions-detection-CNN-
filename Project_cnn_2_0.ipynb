{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Project_cnn_2.0.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Tz7tpDd9_iTp"
      },
      "source": [
        "## Packages and constant declaration"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "qQ-JUWhcCsix",
        "outputId": "4ab8b22e-3716-4017-a6e7-5668732dd906",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "D4fkPcrerF0b",
        "outputId": "93d24eab-36c7-400a-a2a4-5d40ee07d833",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225
        }
      },
      "source": [
        "!apt-get install -y -qq software-properties-common python-software-properties module-init-tools\n",
        "!add-apt-repository -y ppa:alessandro-strada/ppa 2>&1 > /dev/null\n",
        "!apt-get update -qq 2>&1 > /dev/null\n",
        "!apt-get -y install -qq google-drive-ocamlfuse fuse\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "from oauth2client.client import GoogleCredentials\n",
        "creds = GoogleCredentials.get_application_default()\n",
        "import getpass\n",
        "!google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret} < /dev/null 2>&1 | grep URL\n",
        "vcode = getpass.getpass()\n",
        "!echo {vcode} | google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret}"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "E: Package 'python-software-properties' has no installation candidate\n",
            "Selecting previously unselected package google-drive-ocamlfuse.\n",
            "(Reading database ... 131304 files and directories currently installed.)\n",
            "Preparing to unpack .../google-drive-ocamlfuse_0.7.3-0ubuntu3~ubuntu18.04.1_amd64.deb ...\n",
            "Unpacking google-drive-ocamlfuse (0.7.3-0ubuntu3~ubuntu18.04.1) ...\n",
            "Setting up google-drive-ocamlfuse (0.7.3-0ubuntu3~ubuntu18.04.1) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Please, open the following URL in a web browser: https://accounts.google.com/o/oauth2/auth?client_id=32555940559.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&response_type=code&access_type=offline&approval_prompt=force\n",
            "··········\n",
            "Please, open the following URL in a web browser: https://accounts.google.com/o/oauth2/auth?client_id=32555940559.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&response_type=code&access_type=offline&approval_prompt=force\n",
            "Please enter the verification code: Access token retrieved correctly.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "CkoSE2ta_iTr",
        "colab": {}
      },
      "source": [
        "import time"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "uVbTNVII_iTw",
        "outputId": "9e4e17b7-8ab0-4135-a3df-cdf788b8845c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import numpy as np  #linear algebra\n",
        "import pandas as pd #Only CSV IO\n",
        "import os\n",
        "import re\n",
        "from torch.utils import data #dataloader of batch\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn import metrics\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from torch.autograd import Variable #We can ask require grad or not\n",
        "from torch.optim.optimizer import Optimizer\n",
        "from torch.optim import lr_scheduler\n",
        "import torch.nn.functional as F\n",
        "from torchvision import transforms"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "2CxEa0Yy_iT2",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ykgchvANsiTd",
        "outputId": "0347d9a7-a185-460d-816e-557a5da8fe8d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# !ls drive/'My Drive'/'Colab Notebooks'/project\n",
        "# !ls\n",
        "os.chdir(\"drive/My Drive/Colab Notebooks/project\") \n",
        "!ls"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " Cnn.ipynb    Project_cnn_2.0.ipynb    Project_raw.ipynb   train.csv\n",
            " embeddings  'Project_raw (2).ipynb'   test.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "g60Zr6VN_iT5",
        "colab": {}
      },
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "hvfJwrWP_iT8",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler # Simplify the preprocess\n",
        "from multiprocessing import pool #multiprocessing, creating a pool parallel\n",
        "from functools import partial \n",
        "#freeze some arguments of a function to be a new function\n",
        "from sklearn.decomposition import PCA\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "62Jg-3ql_iT-",
        "colab": {}
      },
      "source": [
        "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\" # easy to locate traceback\n",
        "embedding_size = 300 # embedding vector length\n",
        "max_word = 200000 # How many unique words to use \n",
        "max_q = 60 # max number of words in a question\n",
        "maxq=60\n",
        "batch_size = 1024#Batch size\n",
        "n_epochs = 23 # epochs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ErIO6kZ6_iUB",
        "colab": {}
      },
      "source": [
        "def seed(seed=1000):# Give random seed to everything\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "seed()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "n6zdldK5_iUD"
      },
      "source": [
        "## Data preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "VEKwrU5N_iUE",
        "colab": {}
      },
      "source": [
        "puncts = [',', '.', '\"', ':', ')', '(', '-', '!', '?', '|', ';', \"'\", '$', '&', '/', '[', ']', '>', '%', '=', '#', '*', '+', '\\\\', '•',  '~', '@', '£', \n",
        " '·', '_', '{', '}', '©', '^', '®', '`',  '<', '→', '°', '€', '™', '›',  '♥', '←', '×', '§', '″', '′', 'Â', '█', '½', 'à', '…', \n",
        " '“', '★', '”', '–', '●', 'â', '►', '−', '¢', '²', '¬', '░', '¶', '↑', '±', '¿', '▾', '═', '¦', '║', '―', '¥', '▓', '—', '‹', '─', \n",
        " '▒', '：', '¼', '⊕', '▼', '▪', '†', '■', '’', '▀', '¨', '▄', '♫', '☆', 'é', '¯', '♦', '¤', '▲', 'è', '¸', '¾', 'Ã', '⋅', '‘', '∞', \n",
        " '∙', '）', '↓', '、', '│', '（', '»', '，', '♪', '╩', '╚', '³', '・', '╦', '╣', '╔', '╗', '▬', '❤', 'ï', 'Ø', '¹', '≤', '‡', '√', ]\n",
        "\n",
        "def clean(x): #process the punction\n",
        "    x=str(x)\n",
        "    for punct in puncts:\n",
        "        if punct in x:\n",
        "            x=x.replace(punct,f'{punct}')\n",
        "    return x\n",
        "\n",
        "def clean_numbers(x): # Replace the number with #, make all numbers the same\n",
        "    if bool(re.search(r'\\d',x)):\n",
        "        x=re.sub('[0-9]{5,}','#####',x)\n",
        "        x=re.sub('[0-9]{4}','####',x)\n",
        "        x=re.sub('[0-9]{3}','###',x)\n",
        "        x=re.sub('[0-9]{2}','##',x)\n",
        "    return x\n",
        "\n",
        "# The common abbreviation for some phrase\n",
        "fullversiondict={\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\",\n",
        "                 \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\", \"didn't\": \"did not\",  \n",
        "                 \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\",\n",
        "                 \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\",\n",
        "                 \"how's\": \"how is\",  \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\",\n",
        "                 \"I've\": \"I have\", \"i'd\": \"i would\", \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \n",
        "                 \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\", \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\n",
        "                 \"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\", \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \n",
        "                 \"must've\": \"must have\", \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\", \n",
        "                 \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\", \"she'd\": \"she would\", \n",
        "                 \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\", \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \n",
        "                 \"so've\": \"so have\",\"so's\": \"so as\", \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\", \"there'd've\": \"there would have\",\n",
        "                 \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\", \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \n",
        "                 \"to've\": \"to have\", \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\", \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\",\n",
        "                 \"what'll've\": \"what will have\", \"what're\": \"what are\",  \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\", \"where've\": \"where have\", \"who'll\": \"who will\", \n",
        "                 \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\", \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\", \"would've\": \"would have\", \"wouldn't\": \"would not\",\n",
        "                 \"wouldn't've\": \"would not have\", \"y'all\": \"you all\", \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\", \n",
        "                 \"you're\": \"you are\", \"you've\": \"you have\", 'colour': 'color', 'centre': 'center', 'favourite': 'favorite', 'travelling': 'traveling', 'counselling': 'counseling', 'theatre': 'theater', 'cancelled': 'canceled', 'labour': 'labor', 'organisation': 'organization', \n",
        "                 'wwii': 'world war 2', 'citicise': 'criticize', 'youtu ': 'youtube ', 'Qoura': 'Quora', 'sallary': 'salary', 'Whta': 'What', 'narcisist': 'narcissist', 'howdo': 'how do', 'whatare': 'what are', 'howcan': 'how can', 'howmuch': 'how much', 'howmany': 'how many', 'whydo': 'why do',\n",
        "                 'doI': 'do I', 'theBest': 'the best', 'howdoes': 'how does', 'mastrubation': 'masturbation', 'mastrubate': 'masturbate', \"mastrubating\": 'masturbating', 'pennis': 'penis', 'Etherium': 'Ethereum', 'narcissit': 'narcissist', 'bigdata': 'big data', '2k17': '2017', '2k18': '2018',\n",
        "                 'qouta': 'quota','exboyfriend': 'ex boyfriend', 'airhostess': 'air hostess', \"whst\": 'what', 'watsapp': 'whatsapp', 'demonitisation': 'demonetization', 'demonitization': 'demonetization', 'demonetisation': 'demonetization'}\n",
        "def _get_full(fullversiondict):\n",
        "    full_re = re.compile('(%s)'%'|'.join(fullversiondict.keys()))\n",
        "    return fullversiondict , full_re # To make things we want to replace single string\n",
        "\n",
        "fullversiondict,full_re=_get_full(fullversiondict)\n",
        "def replacetext(text):\n",
        "    def replace(match): \n",
        "        return fullversiondict[match.group(0)]\n",
        "    return full_re.sub(replace,text) # The full.re here is match, can replace any string now\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "K1YmE8yu_iUI",
        "colab": {}
      },
      "source": [
        "def read_pre():\n",
        "    train_df = pd.read_csv('train.csv')\n",
        "    test_df = pd.read_csv('test.csv')\n",
        "    print('Train shape:',train_df.shape)\n",
        "    print('Test shape:',test_df.shape)\n",
        "    # Lower case\n",
        "    train_df['question_text'] = train_df['question_text'].apply(lambda x: x.lower())\n",
        "    test_df['question_text'] = test_df['question_text'].apply(lambda x: x.lower())\n",
        "    # Clean punction\n",
        "    train_df[\"question_text\"] = train_df[\"question_text\"].apply(lambda x: clean(x))\n",
        "    test_df[\"question_text\"] = test_df[\"question_text\"].apply(lambda x: clean(x))\n",
        "    #clean numbers\n",
        "    train_df[\"question_text\"] = train_df[\"question_text\"].apply(lambda x: clean_numbers(x))\n",
        "    test_df[\"question_text\"] = test_df[\"question_text\"].apply(lambda x: clean_numbers(x))\n",
        "    #replace abbreviation\n",
        "    train_df[\"question_text\"] = train_df[\"question_text\"].apply(lambda x: replacetext(x))\n",
        "    test_df[\"question_text\"] = test_df[\"question_text\"].apply(lambda x: replacetext(x))\n",
        "    ## fill up the missing values using next valid value\n",
        "    train_X = train_df[\"question_text\"].fillna(method=\"bfill\").values\n",
        "    test_X = test_df[\"question_text\"].fillna(method=\"bfill\").values\n",
        "    \n",
        "    ####################\n",
        "    #Tokenize the sentences\n",
        "    tokenizer = Tokenizer(num_words = max_word)\n",
        "    tokenizer.fit_on_texts(list(train_X)) # only use words in training set\n",
        "    train_X = tokenizer.texts_to_sequences(train_X)\n",
        "    test_X = tokenizer.texts_to_sequences(test_X)\n",
        "    word_index = tokenizer.word_index\n",
        "    nb_words = min(max_word, len(word_index))\n",
        "        \n",
        "    #Pad the sentences, to make equal length of sentences\n",
        "    train_X = pad_sequences(train_X, maxlen=maxq)\n",
        "    test_X = pad_sequences(test_X, maxlen=maxq)\n",
        "     \n",
        "    #Similar to one-hot encoding   \n",
        "    #train_X=np.delete(train_X,[np.where(np.sum(train_X,axis=1)==0)])\n",
        "    #test_X=np.delete(test_X,[np.where(np.sum(test_X,axis=1)==0)])\n",
        "    \n",
        "    #Target\n",
        "    train_y = train_df['target'].values\n",
        "    return train_X, test_X, train_y, word_index\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "RKQrTfMT_iUL",
        "colab": {}
      },
      "source": [
        "class SpatialDropout(nn.Dropout2d):\n",
        "    def forward(self, x):\n",
        "        x = x.unsqueeze(2)    # (N, T, 1, K)\n",
        "        x = x.permute(0, 3, 2, 1)  # (N, K, 1, T)\n",
        "        x = super(SpatialDropout, self).forward(x)  # (N, K, 1, T), some features are masked\n",
        "        x = x.permute(0, 3, 2, 1)  # (N, T, 1, K)\n",
        "        x = x.squeeze(2)  # (N, T, K)\n",
        "        return x\n",
        "#Data augmentation"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "s9oBE4kV_iUN",
        "colab": {}
      },
      "source": [
        "# the pretrained embedding model\n",
        "def load_glove(word_index):\n",
        "    FILE= 'embeddings/glove.840B.300d//glove.840B.300d.txt'\n",
        "    def get_coefs(word,*arr):return word, np.asarray(arr, dtype='float32')[:embedding_size]\n",
        "    embeddings_index = dict(get_coefs(*o.split(' '))for o in open(FILE) if len(o)>300)\n",
        "    #construct word array pair dictionary\n",
        "    all_embs = np.stack(embeddings_index.values())\n",
        "    emb_mean,emb_std = all_embs.mean(), all_embs.std()\n",
        "    nb_words = min(max_word, len(word_index))\n",
        "    #Initialize a matrix using random value, in case that some words don't exist in our embedding\n",
        "    embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words+1,embedding_size))\n",
        "    #Associate the word in our training set with the embedding model\n",
        "    for word, i in word_index.items():\n",
        "        if i>= max_word: continue\n",
        "        embedding_vector = embeddings_index.get(word) # get the vector for this word form dictionary\n",
        "        if embedding_vector is not None: embedding_matrix[i] = embedding_vector\n",
        "    return (embedding_matrix)\n",
        "##Do same thing to other types of embedding\n",
        "\n",
        "def load_para(word_index):\n",
        "    FILE = 'embeddings/paragram_300_sl999-1/paragram_300_sl999.txt'\n",
        "    def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\n",
        "    embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(FILE, encoding=\"utf8\", errors='ignore') if len(o)>300)\n",
        "\n",
        "    all_embs = np.stack(embeddings_index.values())\n",
        "    emb_mean,emb_std = all_embs.mean(), all_embs.std()\n",
        "\n",
        "    nb_words = min(max_word, len(word_index))\n",
        "    embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words+1, embedding_size))\n",
        "    for word, i in word_index.items():\n",
        "        if i >= max_word: continue\n",
        "        embedding_vector = embeddings_index.get(word)\n",
        "        if embedding_vector is not None: embedding_matrix[i] = embedding_vector\n",
        "    \n",
        "    return embedding_matrix\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "xnjBAGru_iUQ",
        "outputId": "969d6477-3c6e-468b-d185-52ac4ea4b3d2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "source": [
        "train_X, test_X, train_y,  word_index = read_pre()\n",
        "embedding_matrix_1 = load_glove(word_index)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train shape: (1306122, 3)\n",
            "Test shape: (375806, 2)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:6: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
            "  \n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "5mLCeCvRJ5yO",
        "outputId": "34acff9e-3e0a-47f8-f080-744752bb10e7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(train_y[train_y==0].shape)\n",
        "# print(train_X[2,])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1225312,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "66r0fbbeJ3J1",
        "outputId": "724c7587-3288-4bec-d508-3f3ef36a29cc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 158
        }
      },
      "source": [
        "##### Structure Trial #####\n",
        "m1=nn.Conv1d(300,128,3)\n",
        "inp=Variable(torch.randn(256,60,300)) #batch*channel*word\n",
        "inp=torch.transpose(inp,1,2)\n",
        "pool=nn.MaxPool1d(4)\n",
        "feature1=m1(inp)\n",
        "feature1=pool(feature1)\n",
        "m2=nn.Conv1d(300,128,4)\n",
        "feature2=m2(inp)\n",
        "feature2=pool(feature2)\n",
        "m3=nn.Conv1d(300,128,5)\n",
        "feature3=m3(inp)\n",
        "feature3=pool(feature3)\n",
        "print(\"feature1\",feature1.size())\n",
        "print(\"feature2\",feature2.size())\n",
        "print(\"feature3\",feature3.size())\n",
        "cat=torch.cat([feature1,feature2,feature3],1)\n",
        "print(\"concat\",cat.size())\n",
        "conv1=nn.Conv1d(384,128,5)\n",
        "out=conv1(cat)\n",
        "out=pool(out)\n",
        "print(\"out\",out.size())\n",
        "conv_2=nn.Conv1d(128,64,2)\n",
        "out=conv_2(out)\n",
        "# pool2=nn.MaxPool1d(2)\n",
        "# out=pool2(out)\n",
        "print(\"out\",out.size())\n",
        "conv_3=nn.Conv1d(64,1,1)\n",
        "out=conv_3(out)\n",
        "print(\"out\",out.size())\n",
        "out=torch.squeeze(out,2)\n",
        "print(\"out\",out.size())\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "feature1 torch.Size([256, 128, 14])\n",
            "feature2 torch.Size([256, 128, 14])\n",
            "feature3 torch.Size([256, 128, 14])\n",
            "concat torch.Size([256, 384, 14])\n",
            "out torch.Size([256, 128, 2])\n",
            "out torch.Size([256, 64, 1])\n",
            "out torch.Size([256, 1, 1])\n",
            "out torch.Size([256, 1])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "UejUcWj5bIsA",
        "colab": {}
      },
      "source": [
        "class NeuralNet2(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(NeuralNet2, self).__init__()\n",
        "        \n",
        "        \n",
        "        self.embedding = nn.Embedding(embedding_size,max_word)\n",
        "        self.embedding.weight = nn.Parameter(torch.tensor(embedding_matrix_1, dtype=torch.float32))\n",
        "        self.embedding.weight.requires_grad = False\n",
        "        #The embedding vectors here are not parameter needing update\n",
        "        self.embedding_dropout = SpatialDropout(0.15)\n",
        "        #Using embedding  and data augmentation\n",
        "         \n",
        "        self.pool = nn.MaxPool1d(4)\n",
        "#         self.pool2 = nn.MaxPool1d(9)\n",
        "        \n",
        "        self.branch1 = nn.Conv1d(300,128,3)        \n",
        "        self.branch2 = nn.Conv1d(300,128,4)\n",
        "        self.branch3 = nn.Conv1d(300,128,5)\n",
        "        \n",
        "        self.bn_branch = nn.BatchNorm1d(128)\n",
        "        self.bn_2 = nn.BatchNorm1d(64)\n",
        "        \n",
        "        self.conv_1=nn.Conv1d(128,64,5)\n",
        "#         self.conv_2=nn.Conv1d(64,64,5)\n",
        "        self.linear=nn.Linear(576,256)\n",
        "        self.linear2=nn.Linear(256,1)\n",
        "      \n",
        "    def forward(self, x):\n",
        "        h_embedding = self.embedding(x)\n",
        "        h_embedding = self.embedding_dropout(h_embedding)#batch*60*300\n",
        "        h_embedding = torch.transpose(h_embedding,1,2) #Conv1D takes batch*channel*width\n",
        "#         print(\"embedding\",h_embedding.size())\n",
        "        #Data augmentation\n",
        "        \n",
        "        branch1=self.branch1(h_embedding)\n",
        "        branch1=self.bn_branch(branch1)\n",
        "        branch1=F.relu(branch1)\n",
        "        branch1 = F.dropout(branch1,p=0.2,training=self.training)\n",
        "        branch1=self.pool(branch1)\n",
        "        branch2=self.branch2(h_embedding)\n",
        "        branch2=self.bn_branch(branch2)\n",
        "        branch2=F.relu(branch2)\n",
        "        branch2 = F.dropout(branch2,p=0.2,training=self.training)\n",
        "        branch2=self.pool(branch2)\n",
        "        branch3=self.branch3(h_embedding)\n",
        "        branch3=self.bn_branch(branch3)\n",
        "        branch3=F.relu(branch3)\n",
        "        branch3 = F.dropout(branch3,p=0.2,training=self.training)\n",
        "        branch3=self.pool(branch3)\n",
        "        cat=torch.cat([branch1,branch2,branch3],2)\n",
        "#         print(\"cat\",cat.size())\n",
        "        out=self.conv_1(cat)\n",
        "        out=self.bn_2(out)\n",
        "        out=F.relu(out)\n",
        "        out=F.dropout(out,p=0.2,training=self.training)\n",
        "        out=self.pool(out)\n",
        "#         out=self.conv_2(out)\n",
        "#         out=self.bn_2(out)\n",
        "#         out=F.relu(out)\n",
        "#         out=F.dropout(out,p=0.2,training=self.training)\n",
        "#         out=self.pool2(out)\n",
        "#         out=self.conv_3(out)\n",
        "#         print(\"out\",out.size())\n",
        "        out=out.view(-1,576)\n",
        "        out=self.linear(out)\n",
        "        out=F.relu(out)\n",
        "        out=self.linear2(out)\n",
        "#         out=torch.squeeze(out,2)\n",
        "        return out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "qSULKcmqsgdF",
        "colab": {}
      },
      "source": [
        "test_X=np.concatenate((train_X[train_y==1][0:10000],train_X[train_y==0][0:10000]))\n",
        "test_y=np.concatenate((train_y[train_y==1][0:10000],train_y[train_y==0][0:10000]))\n",
        "train_XX=np.concatenate((train_X[train_y==1][10000:],train_X[train_y==0][10000:]))\n",
        "train_yy=np.concatenate((train_y[train_y==1][10000:],train_y[train_y==0][10000:]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "cHrOfB22sfs-",
        "colab": {}
      },
      "source": [
        "#### Oversampling ####\n",
        "train_XX_f=train_XX\n",
        "train_yy_f=train_yy\n",
        "for i in range(17):\n",
        "    train_XX_f = np.concatenate((train_XX_f,train_XX[train_yy==1]))\n",
        "    train_yy_f = np.concatenate((train_yy_f,train_yy[train_yy==1]))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "0nKYIMkSTISM",
        "outputId": "1dd494a2-d5f4-4309-c210-88be4e4ffcf8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "print(train_yy_f[train_yy_f==1].shape)\n",
        "print(train_yy_f[train_yy_f==0].shape)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1274580,)\n",
            "(1215312,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "zIUJ5vtG_iU5",
        "outputId": "b9b3f8c7-db4c-4d40-bb63-2e5054d2e4c7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 4540
        }
      },
      "source": [
        "##### NN2(oversample)  #####\n",
        "seed(100)\n",
        "\n",
        "#Train test idx\n",
        "trainfoldx = torch.tensor(train_XX_f, dtype=torch.long).to(device)\n",
        "trainfoldy = torch.tensor(train_yy_f, dtype=torch.float32).to(device)\n",
        "x_val_fold = torch.tensor(test_X, dtype=torch.long).to(device)\n",
        "y_val_fold = torch.tensor(test_y[:,np.newaxis], dtype=torch.float32).to(device)\n",
        "print(trainfoldy[trainfoldy==1].shape)\n",
        "print(trainfoldy[trainfoldy==0].shape)\n",
        "print(y_val_fold[y_val_fold==1].shape)\n",
        "print(y_val_fold[y_val_fold==0].shape)\n",
        "# print(trainfoldy[0:5,])\n",
        "# print(y_val_fold[0:5,])\n",
        "#Our model\n",
        "mbatch_cost = []\n",
        "model = NeuralNet2()\n",
        "model.to(device)\n",
        "loss_fn = torch.nn.BCEWithLogitsLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(),lr=0.001, weight_decay=1e-8)\n",
        "#Dataloader\n",
        "train = torch.utils.data.TensorDataset(trainfoldx,trainfoldy)\n",
        "valid = torch.utils.data.TensorDataset(x_val_fold, y_val_fold)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train , batch_size=batch_size,shuffle=True)\n",
        "valid_loader = torch.utils.data.DataLoader(valid, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    start = time.time()\n",
        "    model.train()\n",
        "    grand_loss = 0\n",
        "    for  batch_idx, (x_batch, y_batch) in enumerate(train_loader):\n",
        "        #forward\n",
        "        y_pred = model(x_batch)\n",
        "        loss = loss_fn(torch.squeeze(y_pred), y_batch)\n",
        "\n",
        "        #backprop\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        mbatch_cost.append(loss.item())\n",
        "        grand_loss += loss.item()/len(train_loader)\n",
        "        if not batch_idx % 256:\n",
        "            print ('Epoch: %03d/%03d | Batch %03d/%03d | Loss: %.4f' \n",
        "               %(epoch+1, n_epochs, batch_idx, len(train_loader), loss.item()))\n",
        "\n",
        "                 #evaluation          \n",
        "    model.eval()\n",
        "\n",
        "    valid_pred = np.array(list())\n",
        "\n",
        "\n",
        "    val_loss = 0\n",
        "\n",
        "    for i, (x_batch, y_batch) in enumerate(valid_loader):\n",
        "        y_pred = model(x_batch).detach()\n",
        "        val_loss += loss_fn(y_pred,y_batch).item()/len(valid_loader)\n",
        "        valid_pred = np.append(valid_pred, np.array(torch.sigmoid(y_pred.cpu()))[:, 0])\n",
        "    epoch_time=time.time()-start\n",
        "    valid_acc = ((valid_pred>0.5).astype(int)==np.array(y_val_fold.squeeze().cpu())).sum()/len(y_val_fold)\n",
        "    print('Epoch {}/{} \\t train_loss={:.4f} \\t test_loss={:.4f}\\t test_accu={:.4f} \\t time={:.2f}s'.format(epoch + 1, n_epochs, grand_loss, val_loss,valid_acc, epoch_time))\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([1274580])\n",
            "torch.Size([1215312])\n",
            "torch.Size([10000])\n",
            "torch.Size([10000])\n",
            "Epoch: 001/023 | Batch 000/2432 | Loss: 0.6803\n",
            "Epoch: 001/023 | Batch 256/2432 | Loss: 0.3192\n",
            "Epoch: 001/023 | Batch 512/2432 | Loss: 0.3279\n",
            "Epoch: 001/023 | Batch 768/2432 | Loss: 0.3151\n",
            "Epoch: 001/023 | Batch 1024/2432 | Loss: 0.3025\n",
            "Epoch: 001/023 | Batch 1280/2432 | Loss: 0.2826\n",
            "Epoch: 001/023 | Batch 1536/2432 | Loss: 0.3021\n",
            "Epoch: 001/023 | Batch 1792/2432 | Loss: 0.2634\n",
            "Epoch: 001/023 | Batch 2048/2432 | Loss: 0.2707\n",
            "Epoch: 001/023 | Batch 2304/2432 | Loss: 0.2599\n",
            "Epoch 1/23 \t train_loss=0.2894 \t test_loss=0.6429\t test_accu=0.5001 \t time=164.64s\n",
            "Epoch: 002/023 | Batch 000/2432 | Loss: 0.2583\n",
            "Epoch: 002/023 | Batch 256/2432 | Loss: 0.2391\n",
            "Epoch: 002/023 | Batch 512/2432 | Loss: 0.2659\n",
            "Epoch: 002/023 | Batch 768/2432 | Loss: 0.2422\n",
            "Epoch: 002/023 | Batch 1024/2432 | Loss: 0.2417\n",
            "Epoch: 002/023 | Batch 1280/2432 | Loss: 0.2542\n",
            "Epoch: 002/023 | Batch 1536/2432 | Loss: 0.2716\n",
            "Epoch: 002/023 | Batch 1792/2432 | Loss: 0.2231\n",
            "Epoch: 002/023 | Batch 2048/2432 | Loss: 0.2436\n",
            "Epoch: 002/023 | Batch 2304/2432 | Loss: 0.2402\n",
            "Epoch 2/23 \t train_loss=0.2478 \t test_loss=0.7689\t test_accu=0.6399 \t time=172.29s\n",
            "Epoch: 003/023 | Batch 000/2432 | Loss: 0.2381\n",
            "Epoch: 003/023 | Batch 256/2432 | Loss: 0.2461\n",
            "Epoch: 003/023 | Batch 512/2432 | Loss: 0.2302\n",
            "Epoch: 003/023 | Batch 768/2432 | Loss: 0.2351\n",
            "Epoch: 003/023 | Batch 1024/2432 | Loss: 0.2203\n",
            "Epoch: 003/023 | Batch 1280/2432 | Loss: 0.2153\n",
            "Epoch: 003/023 | Batch 1536/2432 | Loss: 0.2366\n",
            "Epoch: 003/023 | Batch 1792/2432 | Loss: 0.2265\n",
            "Epoch: 003/023 | Batch 2048/2432 | Loss: 0.1968\n",
            "Epoch: 003/023 | Batch 2304/2432 | Loss: 0.2227\n",
            "Epoch 3/23 \t train_loss=0.2244 \t test_loss=1.3479\t test_accu=0.5131 \t time=171.59s\n",
            "Epoch: 004/023 | Batch 000/2432 | Loss: 0.1981\n",
            "Epoch: 004/023 | Batch 256/2432 | Loss: 0.2323\n",
            "Epoch: 004/023 | Batch 512/2432 | Loss: 0.2018\n",
            "Epoch: 004/023 | Batch 768/2432 | Loss: 0.2074\n",
            "Epoch: 004/023 | Batch 1024/2432 | Loss: 0.2393\n",
            "Epoch: 004/023 | Batch 1280/2432 | Loss: 0.1953\n",
            "Epoch: 004/023 | Batch 1536/2432 | Loss: 0.2262\n",
            "Epoch: 004/023 | Batch 1792/2432 | Loss: 0.1615\n",
            "Epoch: 004/023 | Batch 2048/2432 | Loss: 0.2346\n",
            "Epoch: 004/023 | Batch 2304/2432 | Loss: 0.1938\n",
            "Epoch 4/23 \t train_loss=0.2091 \t test_loss=1.5977\t test_accu=0.5196 \t time=171.15s\n",
            "Epoch: 005/023 | Batch 000/2432 | Loss: 0.1656\n",
            "Epoch: 005/023 | Batch 256/2432 | Loss: 0.1906\n",
            "Epoch: 005/023 | Batch 512/2432 | Loss: 0.2023\n",
            "Epoch: 005/023 | Batch 768/2432 | Loss: 0.1887\n",
            "Epoch: 005/023 | Batch 1024/2432 | Loss: 0.1909\n",
            "Epoch: 005/023 | Batch 1280/2432 | Loss: 0.2181\n",
            "Epoch: 005/023 | Batch 1536/2432 | Loss: 0.2115\n",
            "Epoch: 005/023 | Batch 1792/2432 | Loss: 0.2036\n",
            "Epoch: 005/023 | Batch 2048/2432 | Loss: 0.1973\n",
            "Epoch: 005/023 | Batch 2304/2432 | Loss: 0.1842\n",
            "Epoch 5/23 \t train_loss=0.1993 \t test_loss=0.9937\t test_accu=0.5405 \t time=171.57s\n",
            "Epoch: 006/023 | Batch 000/2432 | Loss: 0.1854\n",
            "Epoch: 006/023 | Batch 256/2432 | Loss: 0.1571\n",
            "Epoch: 006/023 | Batch 512/2432 | Loss: 0.1783\n",
            "Epoch: 006/023 | Batch 768/2432 | Loss: 0.1973\n",
            "Epoch: 006/023 | Batch 1024/2432 | Loss: 0.1794\n",
            "Epoch: 006/023 | Batch 1280/2432 | Loss: 0.1712\n",
            "Epoch: 006/023 | Batch 1536/2432 | Loss: 0.2080\n",
            "Epoch: 006/023 | Batch 1792/2432 | Loss: 0.1941\n",
            "Epoch: 006/023 | Batch 2048/2432 | Loss: 0.1800\n",
            "Epoch: 006/023 | Batch 2304/2432 | Loss: 0.2036\n",
            "Epoch 6/23 \t train_loss=0.1918 \t test_loss=0.7957\t test_accu=0.6333 \t time=171.08s\n",
            "Epoch: 007/023 | Batch 000/2432 | Loss: 0.1757\n",
            "Epoch: 007/023 | Batch 256/2432 | Loss: 0.2001\n",
            "Epoch: 007/023 | Batch 512/2432 | Loss: 0.1971\n",
            "Epoch: 007/023 | Batch 768/2432 | Loss: 0.2096\n",
            "Epoch: 007/023 | Batch 1024/2432 | Loss: 0.1866\n",
            "Epoch: 007/023 | Batch 1280/2432 | Loss: 0.2020\n",
            "Epoch: 007/023 | Batch 1536/2432 | Loss: 0.2088\n",
            "Epoch: 007/023 | Batch 1792/2432 | Loss: 0.1921\n",
            "Epoch: 007/023 | Batch 2048/2432 | Loss: 0.1829\n",
            "Epoch: 007/023 | Batch 2304/2432 | Loss: 0.2076\n",
            "Epoch 7/23 \t train_loss=0.1867 \t test_loss=1.1144\t test_accu=0.5952 \t time=171.08s\n",
            "Epoch: 008/023 | Batch 000/2432 | Loss: 0.1639\n",
            "Epoch: 008/023 | Batch 256/2432 | Loss: 0.1972\n",
            "Epoch: 008/023 | Batch 512/2432 | Loss: 0.1908\n",
            "Epoch: 008/023 | Batch 768/2432 | Loss: 0.1649\n",
            "Epoch: 008/023 | Batch 1024/2432 | Loss: 0.1938\n",
            "Epoch: 008/023 | Batch 1280/2432 | Loss: 0.1832\n",
            "Epoch: 008/023 | Batch 1536/2432 | Loss: 0.1848\n",
            "Epoch: 008/023 | Batch 1792/2432 | Loss: 0.1540\n",
            "Epoch: 008/023 | Batch 2048/2432 | Loss: 0.1972\n",
            "Epoch: 008/023 | Batch 2304/2432 | Loss: 0.2098\n",
            "Epoch 8/23 \t train_loss=0.1823 \t test_loss=0.5998\t test_accu=0.7936 \t time=170.62s\n",
            "Epoch: 009/023 | Batch 000/2432 | Loss: 0.1770\n",
            "Epoch: 009/023 | Batch 256/2432 | Loss: 0.1651\n",
            "Epoch: 009/023 | Batch 512/2432 | Loss: 0.1691\n",
            "Epoch: 009/023 | Batch 768/2432 | Loss: 0.1687\n",
            "Epoch: 009/023 | Batch 1024/2432 | Loss: 0.1421\n",
            "Epoch: 009/023 | Batch 1280/2432 | Loss: 0.1832\n",
            "Epoch: 009/023 | Batch 1536/2432 | Loss: 0.1917\n",
            "Epoch: 009/023 | Batch 1792/2432 | Loss: 0.2149\n",
            "Epoch: 009/023 | Batch 2048/2432 | Loss: 0.1609\n",
            "Epoch: 009/023 | Batch 2304/2432 | Loss: 0.1899\n",
            "Epoch 9/23 \t train_loss=0.1788 \t test_loss=0.3246\t test_accu=0.8802 \t time=170.93s\n",
            "Epoch: 010/023 | Batch 000/2432 | Loss: 0.1774\n",
            "Epoch: 010/023 | Batch 256/2432 | Loss: 0.1782\n",
            "Epoch: 010/023 | Batch 512/2432 | Loss: 0.2122\n",
            "Epoch: 010/023 | Batch 768/2432 | Loss: 0.1708\n",
            "Epoch: 010/023 | Batch 1024/2432 | Loss: 0.1752\n",
            "Epoch: 010/023 | Batch 1280/2432 | Loss: 0.1836\n",
            "Epoch: 010/023 | Batch 1536/2432 | Loss: 0.1917\n",
            "Epoch: 010/023 | Batch 1792/2432 | Loss: 0.1773\n",
            "Epoch: 010/023 | Batch 2048/2432 | Loss: 0.1705\n",
            "Epoch: 010/023 | Batch 2304/2432 | Loss: 0.1878\n",
            "Epoch 10/23 \t train_loss=0.1759 \t test_loss=0.3271\t test_accu=0.8461 \t time=170.90s\n",
            "Epoch: 011/023 | Batch 000/2432 | Loss: 0.1889\n",
            "Epoch: 011/023 | Batch 256/2432 | Loss: 0.1781\n",
            "Epoch: 011/023 | Batch 512/2432 | Loss: 0.1872\n",
            "Epoch: 011/023 | Batch 768/2432 | Loss: 0.1534\n",
            "Epoch: 011/023 | Batch 1024/2432 | Loss: 0.1813\n",
            "Epoch: 011/023 | Batch 1280/2432 | Loss: 0.1920\n",
            "Epoch: 011/023 | Batch 1536/2432 | Loss: 0.2116\n",
            "Epoch: 011/023 | Batch 1792/2432 | Loss: 0.1626\n",
            "Epoch: 011/023 | Batch 2048/2432 | Loss: 0.1838\n",
            "Epoch: 011/023 | Batch 2304/2432 | Loss: 0.1782\n",
            "Epoch 11/23 \t train_loss=0.1737 \t test_loss=0.4183\t test_accu=0.7544 \t time=171.26s\n",
            "Epoch: 012/023 | Batch 000/2432 | Loss: 0.1343\n",
            "Epoch: 012/023 | Batch 256/2432 | Loss: 0.1714\n",
            "Epoch: 012/023 | Batch 512/2432 | Loss: 0.1628\n",
            "Epoch: 012/023 | Batch 768/2432 | Loss: 0.1830\n",
            "Epoch: 012/023 | Batch 1024/2432 | Loss: 0.1904\n",
            "Epoch: 012/023 | Batch 1280/2432 | Loss: 0.1489\n",
            "Epoch: 012/023 | Batch 1536/2432 | Loss: 0.1647\n",
            "Epoch: 012/023 | Batch 1792/2432 | Loss: 0.1479\n",
            "Epoch: 012/023 | Batch 2048/2432 | Loss: 0.1766\n",
            "Epoch: 012/023 | Batch 2304/2432 | Loss: 0.1633\n",
            "Epoch 12/23 \t train_loss=0.1715 \t test_loss=0.4233\t test_accu=0.7730 \t time=170.83s\n",
            "Epoch: 013/023 | Batch 000/2432 | Loss: 0.1589\n",
            "Epoch: 013/023 | Batch 256/2432 | Loss: 0.1567\n",
            "Epoch: 013/023 | Batch 512/2432 | Loss: 0.1715\n",
            "Epoch: 013/023 | Batch 768/2432 | Loss: 0.1681\n",
            "Epoch: 013/023 | Batch 1024/2432 | Loss: 0.1544\n",
            "Epoch: 013/023 | Batch 1280/2432 | Loss: 0.1929\n",
            "Epoch: 013/023 | Batch 1536/2432 | Loss: 0.1757\n",
            "Epoch: 013/023 | Batch 1792/2432 | Loss: 0.1732\n",
            "Epoch: 013/023 | Batch 2048/2432 | Loss: 0.1675\n",
            "Epoch: 013/023 | Batch 2304/2432 | Loss: 0.1519\n",
            "Epoch 13/23 \t train_loss=0.1695 \t test_loss=0.4671\t test_accu=0.7583 \t time=170.76s\n",
            "Epoch: 014/023 | Batch 000/2432 | Loss: 0.1647\n",
            "Epoch: 014/023 | Batch 256/2432 | Loss: 0.1713\n",
            "Epoch: 014/023 | Batch 512/2432 | Loss: 0.1495\n",
            "Epoch: 014/023 | Batch 768/2432 | Loss: 0.1550\n",
            "Epoch: 014/023 | Batch 1024/2432 | Loss: 0.1767\n",
            "Epoch: 014/023 | Batch 1280/2432 | Loss: 0.1517\n",
            "Epoch: 014/023 | Batch 1536/2432 | Loss: 0.1559\n",
            "Epoch: 014/023 | Batch 1792/2432 | Loss: 0.1475\n",
            "Epoch: 014/023 | Batch 2048/2432 | Loss: 0.1640\n",
            "Epoch: 014/023 | Batch 2304/2432 | Loss: 0.1612\n",
            "Epoch 14/23 \t train_loss=0.1669 \t test_loss=0.6437\t test_accu=0.6369 \t time=170.75s\n",
            "Epoch: 015/023 | Batch 000/2432 | Loss: 0.1632\n",
            "Epoch: 015/023 | Batch 256/2432 | Loss: 0.1818\n",
            "Epoch: 015/023 | Batch 512/2432 | Loss: 0.1673\n",
            "Epoch: 015/023 | Batch 768/2432 | Loss: 0.1691\n",
            "Epoch: 015/023 | Batch 1024/2432 | Loss: 0.1863\n",
            "Epoch: 015/023 | Batch 1280/2432 | Loss: 0.1728\n",
            "Epoch: 015/023 | Batch 1536/2432 | Loss: 0.1725\n",
            "Epoch: 015/023 | Batch 1792/2432 | Loss: 0.1404\n",
            "Epoch: 015/023 | Batch 2048/2432 | Loss: 0.1493\n",
            "Epoch: 015/023 | Batch 2304/2432 | Loss: 0.1724\n",
            "Epoch 15/23 \t train_loss=0.1650 \t test_loss=0.5508\t test_accu=0.6784 \t time=170.91s\n",
            "Epoch: 016/023 | Batch 000/2432 | Loss: 0.1622\n",
            "Epoch: 016/023 | Batch 256/2432 | Loss: 0.1810\n",
            "Epoch: 016/023 | Batch 512/2432 | Loss: 0.1667\n",
            "Epoch: 016/023 | Batch 768/2432 | Loss: 0.1816\n",
            "Epoch: 016/023 | Batch 1024/2432 | Loss: 0.1668\n",
            "Epoch: 016/023 | Batch 1280/2432 | Loss: 0.1748\n",
            "Epoch: 016/023 | Batch 1536/2432 | Loss: 0.1582\n",
            "Epoch: 016/023 | Batch 1792/2432 | Loss: 0.1622\n",
            "Epoch: 016/023 | Batch 2048/2432 | Loss: 0.1552\n",
            "Epoch: 016/023 | Batch 2304/2432 | Loss: 0.1556\n",
            "Epoch 16/23 \t train_loss=0.1636 \t test_loss=0.4419\t test_accu=0.7909 \t time=170.97s\n",
            "Epoch: 017/023 | Batch 000/2432 | Loss: 0.1756\n",
            "Epoch: 017/023 | Batch 256/2432 | Loss: 0.1605\n",
            "Epoch: 017/023 | Batch 512/2432 | Loss: 0.1693\n",
            "Epoch: 017/023 | Batch 768/2432 | Loss: 0.1606\n",
            "Epoch: 017/023 | Batch 1024/2432 | Loss: 0.1549\n",
            "Epoch: 017/023 | Batch 1280/2432 | Loss: 0.1459\n",
            "Epoch: 017/023 | Batch 1536/2432 | Loss: 0.1337\n",
            "Epoch: 017/023 | Batch 1792/2432 | Loss: 0.1584\n",
            "Epoch: 017/023 | Batch 2048/2432 | Loss: 0.1490\n",
            "Epoch: 017/023 | Batch 2304/2432 | Loss: 0.1729\n",
            "Epoch 17/23 \t train_loss=0.1618 \t test_loss=0.4131\t test_accu=0.8134 \t time=170.68s\n",
            "Epoch: 018/023 | Batch 000/2432 | Loss: 0.1252\n",
            "Epoch: 018/023 | Batch 256/2432 | Loss: 0.1782\n",
            "Epoch: 018/023 | Batch 512/2432 | Loss: 0.1638\n",
            "Epoch: 018/023 | Batch 768/2432 | Loss: 0.1393\n",
            "Epoch: 018/023 | Batch 1024/2432 | Loss: 0.1558\n",
            "Epoch: 018/023 | Batch 1280/2432 | Loss: 0.1478\n",
            "Epoch: 018/023 | Batch 1536/2432 | Loss: 0.1373\n",
            "Epoch: 018/023 | Batch 1792/2432 | Loss: 0.1519\n",
            "Epoch: 018/023 | Batch 2048/2432 | Loss: 0.1465\n",
            "Epoch: 018/023 | Batch 2304/2432 | Loss: 0.1838\n",
            "Epoch 18/23 \t train_loss=0.1605 \t test_loss=0.3812\t test_accu=0.8310 \t time=170.80s\n",
            "Epoch: 019/023 | Batch 000/2432 | Loss: 0.1695\n",
            "Epoch: 019/023 | Batch 256/2432 | Loss: 0.1608\n",
            "Epoch: 019/023 | Batch 512/2432 | Loss: 0.1640\n",
            "Epoch: 019/023 | Batch 768/2432 | Loss: 0.1702\n",
            "Epoch: 019/023 | Batch 1024/2432 | Loss: 0.1854\n",
            "Epoch: 019/023 | Batch 1280/2432 | Loss: 0.1820\n",
            "Epoch: 019/023 | Batch 1536/2432 | Loss: 0.1721\n",
            "Epoch: 019/023 | Batch 1792/2432 | Loss: 0.1374\n",
            "Epoch: 019/023 | Batch 2048/2432 | Loss: 0.1442\n",
            "Epoch: 019/023 | Batch 2304/2432 | Loss: 0.1608\n",
            "Epoch 19/23 \t train_loss=0.1596 \t test_loss=0.4159\t test_accu=0.8038 \t time=170.78s\n",
            "Epoch: 020/023 | Batch 000/2432 | Loss: 0.1752\n",
            "Epoch: 020/023 | Batch 256/2432 | Loss: 0.1614\n",
            "Epoch: 020/023 | Batch 512/2432 | Loss: 0.1619\n",
            "Epoch: 020/023 | Batch 768/2432 | Loss: 0.1304\n",
            "Epoch: 020/023 | Batch 1024/2432 | Loss: 0.1560\n",
            "Epoch: 020/023 | Batch 1280/2432 | Loss: 0.1234\n",
            "Epoch: 020/023 | Batch 1536/2432 | Loss: 0.1790\n",
            "Epoch: 020/023 | Batch 1792/2432 | Loss: 0.1420\n",
            "Epoch: 020/023 | Batch 2048/2432 | Loss: 0.2021\n",
            "Epoch: 020/023 | Batch 2304/2432 | Loss: 0.1630\n",
            "Epoch 20/23 \t train_loss=0.1586 \t test_loss=0.4335\t test_accu=0.8009 \t time=170.77s\n",
            "Epoch: 021/023 | Batch 000/2432 | Loss: 0.1356\n",
            "Epoch: 021/023 | Batch 256/2432 | Loss: 0.1690\n",
            "Epoch: 021/023 | Batch 512/2432 | Loss: 0.1705\n",
            "Epoch: 021/023 | Batch 768/2432 | Loss: 0.1497\n",
            "Epoch: 021/023 | Batch 1024/2432 | Loss: 0.1618\n",
            "Epoch: 021/023 | Batch 1280/2432 | Loss: 0.1496\n",
            "Epoch: 021/023 | Batch 1536/2432 | Loss: 0.1342\n",
            "Epoch: 021/023 | Batch 1792/2432 | Loss: 0.1518\n",
            "Epoch: 021/023 | Batch 2048/2432 | Loss: 0.1845\n",
            "Epoch: 021/023 | Batch 2304/2432 | Loss: 0.1385\n",
            "Epoch 21/23 \t train_loss=0.1571 \t test_loss=0.3679\t test_accu=0.8486 \t time=170.99s\n",
            "Epoch: 022/023 | Batch 000/2432 | Loss: 0.1729\n",
            "Epoch: 022/023 | Batch 256/2432 | Loss: 0.1566\n",
            "Epoch: 022/023 | Batch 512/2432 | Loss: 0.1340\n",
            "Epoch: 022/023 | Batch 768/2432 | Loss: 0.1522\n",
            "Epoch: 022/023 | Batch 1024/2432 | Loss: 0.1564\n",
            "Epoch: 022/023 | Batch 1280/2432 | Loss: 0.1569\n",
            "Epoch: 022/023 | Batch 1536/2432 | Loss: 0.1603\n",
            "Epoch: 022/023 | Batch 1792/2432 | Loss: 0.1485\n",
            "Epoch: 022/023 | Batch 2048/2432 | Loss: 0.1560\n",
            "Epoch: 022/023 | Batch 2304/2432 | Loss: 0.1608\n",
            "Epoch 22/23 \t train_loss=0.1566 \t test_loss=0.3545\t test_accu=0.8668 \t time=171.12s\n",
            "Epoch: 023/023 | Batch 000/2432 | Loss: 0.1415\n",
            "Epoch: 023/023 | Batch 256/2432 | Loss: 0.1807\n",
            "Epoch: 023/023 | Batch 512/2432 | Loss: 0.1629\n",
            "Epoch: 023/023 | Batch 768/2432 | Loss: 0.1625\n",
            "Epoch: 023/023 | Batch 1024/2432 | Loss: 0.1596\n",
            "Epoch: 023/023 | Batch 1280/2432 | Loss: 0.1557\n",
            "Epoch: 023/023 | Batch 1536/2432 | Loss: 0.1488\n",
            "Epoch: 023/023 | Batch 1792/2432 | Loss: 0.1549\n",
            "Epoch: 023/023 | Batch 2048/2432 | Loss: 0.1661\n",
            "Epoch: 023/023 | Batch 2304/2432 | Loss: 0.1709\n",
            "Epoch 23/23 \t train_loss=0.1556 \t test_loss=0.3887\t test_accu=0.8442 \t time=170.90s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "9QrBXwIVpZuT",
        "outputId": "f7d9f0ab-dce7-4e95-c74d-dc406d406f9c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 287
        }
      },
      "source": [
        "plt.plot(mbatch_cost)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f3136b75898>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n/wFA8vq2Lie+xQAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b_mMI_agoAH5",
        "colab_type": "code",
        "outputId": "4eeab520-907e-429e-a797-1c74ebd4778d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "#### Train set retrieve ####\n",
        "train_XX_f=train_XX\n",
        "train_yy_f=train_yy\n",
        "print(train_yy_f[train_yy_f==1].shape)\n",
        "print(train_yy_f[train_yy_f==0].shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(70810,)\n",
            "(1215312,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kN0wxoH_2tzS",
        "colab_type": "code",
        "outputId": "0492aa53-6c8a-4dcc-cbc4-0a6d9c04f29f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2440
        }
      },
      "source": [
        "#### NN2 loss biased ####\n",
        "seed(100)\n",
        "\n",
        "#Train test idx\n",
        "trainfoldx = torch.tensor(train_XX_f, dtype=torch.long).to(device)\n",
        "trainfoldy = torch.tensor(train_yy_f, dtype=torch.float32).to(device)\n",
        "x_val_fold = torch.tensor(test_X, dtype=torch.long).to(device)\n",
        "y_val_fold = torch.tensor(test_y[:,np.newaxis], dtype=torch.float32).to(device)\n",
        "print(trainfoldy[trainfoldy==1].shape)\n",
        "print(trainfoldy[trainfoldy==0].shape)\n",
        "print(y_val_fold[y_val_fold==1].shape)\n",
        "print(y_val_fold[y_val_fold==0].shape)\n",
        "# print(trainfoldy[0:5,])\n",
        "# print(y_val_fold[0:5,])\n",
        "#Our model\n",
        "mbatch_cost = []\n",
        "model = NeuralNet2()\n",
        "model.to(device)\n",
        "loss_fn = torch.nn.BCEWithLogitsLoss(pos_weight=torch.Tensor([(17.0)]).to(device))\n",
        "optimizer = torch.optim.Adam(model.parameters(),lr=0.001, weight_decay=1e-8)\n",
        "#Dataloader\n",
        "train = torch.utils.data.TensorDataset(trainfoldx,trainfoldy)\n",
        "valid = torch.utils.data.TensorDataset(x_val_fold, y_val_fold)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train , batch_size=batch_size,shuffle=True)\n",
        "valid_loader = torch.utils.data.DataLoader(valid, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    start = time.time()\n",
        "    model.train()\n",
        "    grand_loss = 0\n",
        "    for  batch_idx, (x_batch, y_batch) in enumerate(train_loader):\n",
        "        #forward\n",
        "        y_pred = model(x_batch)\n",
        "        loss = loss_fn(torch.squeeze(y_pred), y_batch)\n",
        "\n",
        "        #backprop\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        mbatch_cost.append(loss.item())\n",
        "        grand_loss += loss.item()/len(train_loader)\n",
        "        if not batch_idx % 256:\n",
        "            print ('Epoch: %03d/%03d | Batch %03d/%03d | Loss: %.4f' \n",
        "               %(epoch+1, n_epochs, batch_idx, len(train_loader), loss.item()))\n",
        "\n",
        "                 #evaluation          \n",
        "    model.eval()\n",
        "\n",
        "    valid_pred = np.array(list())\n",
        "\n",
        "\n",
        "    val_loss = 0\n",
        "\n",
        "    for i, (x_batch, y_batch) in enumerate(valid_loader):\n",
        "        y_pred = model(x_batch).detach()\n",
        "        val_loss += loss_fn(y_pred,y_batch).item()/len(valid_loader)\n",
        "        valid_pred = np.append(valid_pred, np.array(torch.sigmoid(y_pred.cpu()))[:, 0])\n",
        "    epoch_time=time.time()-start\n",
        "    valid_acc = ((valid_pred>0.5).astype(int)==np.array(y_val_fold.squeeze().cpu())).sum()/len(y_val_fold)\n",
        "    print('Epoch {}/{} \\t train_loss={:.4f} \\t test_loss={:.4f}\\t test_accu={:.4f} \\t time={:.2f}s'.format(epoch + 1, n_epochs, grand_loss, val_loss,valid_acc, epoch_time))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([70810])\n",
            "torch.Size([1215312])\n",
            "torch.Size([10000])\n",
            "torch.Size([10000])\n",
            "Epoch: 001/023 | Batch 000/1256 | Loss: 1.2228\n",
            "Epoch: 001/023 | Batch 256/1256 | Loss: 0.7291\n",
            "Epoch: 001/023 | Batch 512/1256 | Loss: 0.6804\n",
            "Epoch: 001/023 | Batch 768/1256 | Loss: 0.5051\n",
            "Epoch: 001/023 | Batch 1024/1256 | Loss: 0.5618\n",
            "Epoch 1/23 \t train_loss=0.6353 \t test_loss=10.1508\t test_accu=0.5000 \t time=85.57s\n",
            "Epoch: 002/023 | Batch 000/1256 | Loss: 0.7217\n",
            "Epoch: 002/023 | Batch 256/1256 | Loss: 0.7109\n",
            "Epoch: 002/023 | Batch 512/1256 | Loss: 0.7309\n",
            "Epoch: 002/023 | Batch 768/1256 | Loss: 0.8359\n",
            "Epoch: 002/023 | Batch 1024/1256 | Loss: 0.5528\n",
            "Epoch 2/23 \t train_loss=0.5897 \t test_loss=3.5950\t test_accu=0.4999 \t time=90.04s\n",
            "Epoch: 003/023 | Batch 000/1256 | Loss: 0.6825\n",
            "Epoch: 003/023 | Batch 256/1256 | Loss: 0.5546\n",
            "Epoch: 003/023 | Batch 512/1256 | Loss: 0.5763\n",
            "Epoch: 003/023 | Batch 768/1256 | Loss: 0.7653\n",
            "Epoch: 003/023 | Batch 1024/1256 | Loss: 0.7301\n",
            "Epoch 3/23 \t train_loss=0.5721 \t test_loss=5.9299\t test_accu=0.4941 \t time=91.58s\n",
            "Epoch: 004/023 | Batch 000/1256 | Loss: 0.6220\n",
            "Epoch: 004/023 | Batch 256/1256 | Loss: 0.5941\n",
            "Epoch: 004/023 | Batch 512/1256 | Loss: 0.4942\n",
            "Epoch: 004/023 | Batch 768/1256 | Loss: 0.5435\n",
            "Epoch: 004/023 | Batch 1024/1256 | Loss: 0.4753\n",
            "Epoch 4/23 \t train_loss=0.5586 \t test_loss=4.1891\t test_accu=0.4886 \t time=91.49s\n",
            "Epoch: 005/023 | Batch 000/1256 | Loss: 0.4568\n",
            "Epoch: 005/023 | Batch 256/1256 | Loss: 0.4788\n",
            "Epoch: 005/023 | Batch 512/1256 | Loss: 0.5339\n",
            "Epoch: 005/023 | Batch 768/1256 | Loss: 0.4419\n",
            "Epoch: 005/023 | Batch 1024/1256 | Loss: 0.5990\n",
            "Epoch 5/23 \t train_loss=0.5451 \t test_loss=4.9628\t test_accu=0.4923 \t time=91.18s\n",
            "Epoch: 006/023 | Batch 000/1256 | Loss: 0.5970\n",
            "Epoch: 006/023 | Batch 256/1256 | Loss: 0.6058\n",
            "Epoch: 006/023 | Batch 512/1256 | Loss: 0.6222\n",
            "Epoch: 006/023 | Batch 768/1256 | Loss: 0.5568\n",
            "Epoch: 006/023 | Batch 1024/1256 | Loss: 0.4047\n",
            "Epoch 6/23 \t train_loss=0.5352 \t test_loss=5.6598\t test_accu=0.5084 \t time=91.69s\n",
            "Epoch: 007/023 | Batch 000/1256 | Loss: 0.5659\n",
            "Epoch: 007/023 | Batch 256/1256 | Loss: 0.6217\n",
            "Epoch: 007/023 | Batch 512/1256 | Loss: 0.4392\n",
            "Epoch: 007/023 | Batch 768/1256 | Loss: 0.5799\n",
            "Epoch: 007/023 | Batch 1024/1256 | Loss: 0.6483\n",
            "Epoch 7/23 \t train_loss=0.5262 \t test_loss=6.7914\t test_accu=0.5939 \t time=91.15s\n",
            "Epoch: 008/023 | Batch 000/1256 | Loss: 0.4342\n",
            "Epoch: 008/023 | Batch 256/1256 | Loss: 0.4125\n",
            "Epoch: 008/023 | Batch 512/1256 | Loss: 0.5349\n",
            "Epoch: 008/023 | Batch 768/1256 | Loss: 0.4524\n",
            "Epoch: 008/023 | Batch 1024/1256 | Loss: 0.5634\n",
            "Epoch 8/23 \t train_loss=0.5178 \t test_loss=10.9271\t test_accu=0.5000 \t time=90.91s\n",
            "Epoch: 009/023 | Batch 000/1256 | Loss: 0.3975\n",
            "Epoch: 009/023 | Batch 256/1256 | Loss: 0.5260\n",
            "Epoch: 009/023 | Batch 512/1256 | Loss: 0.6022\n",
            "Epoch: 009/023 | Batch 768/1256 | Loss: 0.6647\n",
            "Epoch: 009/023 | Batch 1024/1256 | Loss: 0.3707\n",
            "Epoch 9/23 \t train_loss=0.5108 \t test_loss=12.4525\t test_accu=0.5000 \t time=90.74s\n",
            "Epoch: 010/023 | Batch 000/1256 | Loss: 0.4805\n",
            "Epoch: 010/023 | Batch 256/1256 | Loss: 0.4952\n",
            "Epoch: 010/023 | Batch 512/1256 | Loss: 0.5123\n",
            "Epoch: 010/023 | Batch 768/1256 | Loss: 0.6807\n",
            "Epoch: 010/023 | Batch 1024/1256 | Loss: 0.5386\n",
            "Epoch 10/23 \t train_loss=0.5037 \t test_loss=13.0555\t test_accu=0.5000 \t time=90.88s\n",
            "Epoch: 011/023 | Batch 000/1256 | Loss: 0.5609\n",
            "Epoch: 011/023 | Batch 256/1256 | Loss: 0.5259\n",
            "Epoch: 011/023 | Batch 512/1256 | Loss: 0.5489\n",
            "Epoch: 011/023 | Batch 768/1256 | Loss: 0.5602\n",
            "Epoch: 011/023 | Batch 1024/1256 | Loss: 0.4466\n",
            "Epoch 11/23 \t train_loss=0.4982 \t test_loss=16.2328\t test_accu=0.5000 \t time=90.39s\n",
            "Epoch: 012/023 | Batch 000/1256 | Loss: 0.4651\n",
            "Epoch: 012/023 | Batch 256/1256 | Loss: 0.5746\n",
            "Epoch: 012/023 | Batch 512/1256 | Loss: 0.4188\n",
            "Epoch: 012/023 | Batch 768/1256 | Loss: 0.5512\n",
            "Epoch: 012/023 | Batch 1024/1256 | Loss: 0.5004\n",
            "Epoch 12/23 \t train_loss=0.4918 \t test_loss=25.3371\t test_accu=0.5000 \t time=90.46s\n",
            "Epoch: 013/023 | Batch 000/1256 | Loss: 0.4051\n",
            "Epoch: 013/023 | Batch 256/1256 | Loss: 0.5267\n",
            "Epoch: 013/023 | Batch 512/1256 | Loss: 0.5064\n",
            "Epoch: 013/023 | Batch 768/1256 | Loss: 0.4807\n",
            "Epoch: 013/023 | Batch 1024/1256 | Loss: 0.5022\n",
            "Epoch 13/23 \t train_loss=0.4857 \t test_loss=22.9263\t test_accu=0.5000 \t time=90.58s\n",
            "Epoch: 014/023 | Batch 000/1256 | Loss: 0.5699\n",
            "Epoch: 014/023 | Batch 256/1256 | Loss: 0.5382\n",
            "Epoch: 014/023 | Batch 512/1256 | Loss: 0.4390\n",
            "Epoch: 014/023 | Batch 768/1256 | Loss: 0.5554\n",
            "Epoch: 014/023 | Batch 1024/1256 | Loss: 0.4752\n",
            "Epoch 14/23 \t train_loss=0.4803 \t test_loss=25.6136\t test_accu=0.5000 \t time=90.41s\n",
            "Epoch: 015/023 | Batch 000/1256 | Loss: 0.4483\n",
            "Epoch: 015/023 | Batch 256/1256 | Loss: 0.6954\n",
            "Epoch: 015/023 | Batch 512/1256 | Loss: 0.4754\n",
            "Epoch: 015/023 | Batch 768/1256 | Loss: 0.4076\n",
            "Epoch: 015/023 | Batch 1024/1256 | Loss: 0.4133\n",
            "Epoch 15/23 \t train_loss=0.4728 \t test_loss=20.1851\t test_accu=0.5000 \t time=90.66s\n",
            "Epoch: 016/023 | Batch 000/1256 | Loss: 0.5769\n",
            "Epoch: 016/023 | Batch 256/1256 | Loss: 0.3859\n",
            "Epoch: 016/023 | Batch 512/1256 | Loss: 0.5147\n",
            "Epoch: 016/023 | Batch 768/1256 | Loss: 0.3620\n",
            "Epoch: 016/023 | Batch 1024/1256 | Loss: 0.6835\n",
            "Epoch 16/23 \t train_loss=0.4701 \t test_loss=15.1129\t test_accu=0.5007 \t time=91.07s\n",
            "Epoch: 017/023 | Batch 000/1256 | Loss: 0.5069\n",
            "Epoch: 017/023 | Batch 256/1256 | Loss: 0.4378\n",
            "Epoch: 017/023 | Batch 512/1256 | Loss: 0.3881\n",
            "Epoch: 017/023 | Batch 768/1256 | Loss: 0.4091\n",
            "Epoch: 017/023 | Batch 1024/1256 | Loss: 0.5466\n",
            "Epoch 17/23 \t train_loss=0.4656 \t test_loss=12.1562\t test_accu=0.5292 \t time=91.32s\n",
            "Epoch: 018/023 | Batch 000/1256 | Loss: 0.4508\n",
            "Epoch: 018/023 | Batch 256/1256 | Loss: 0.5336\n",
            "Epoch: 018/023 | Batch 512/1256 | Loss: 0.4149\n",
            "Epoch: 018/023 | Batch 768/1256 | Loss: 0.3956\n",
            "Epoch: 018/023 | Batch 1024/1256 | Loss: 0.4880\n",
            "Epoch 18/23 \t train_loss=0.4613 \t test_loss=8.5329\t test_accu=0.6962 \t time=91.06s\n",
            "Epoch: 019/023 | Batch 000/1256 | Loss: 0.4643\n",
            "Epoch: 019/023 | Batch 256/1256 | Loss: 0.4434\n",
            "Epoch: 019/023 | Batch 512/1256 | Loss: 0.3937\n",
            "Epoch: 019/023 | Batch 768/1256 | Loss: 0.4164\n",
            "Epoch: 019/023 | Batch 1024/1256 | Loss: 0.3986\n",
            "Epoch 19/23 \t train_loss=0.4564 \t test_loss=4.1922\t test_accu=0.8529 \t time=91.22s\n",
            "Epoch: 020/023 | Batch 000/1256 | Loss: 0.3864\n",
            "Epoch: 020/023 | Batch 256/1256 | Loss: 0.4096\n",
            "Epoch: 020/023 | Batch 512/1256 | Loss: 0.3481\n",
            "Epoch: 020/023 | Batch 768/1256 | Loss: 0.3412\n",
            "Epoch: 020/023 | Batch 1024/1256 | Loss: 0.6011\n",
            "Epoch 20/23 \t train_loss=0.4529 \t test_loss=3.8455\t test_accu=0.8568 \t time=91.41s\n",
            "Epoch: 021/023 | Batch 000/1256 | Loss: 0.6349\n",
            "Epoch: 021/023 | Batch 256/1256 | Loss: 0.5571\n",
            "Epoch: 021/023 | Batch 512/1256 | Loss: 0.4223\n",
            "Epoch: 021/023 | Batch 768/1256 | Loss: 0.4311\n",
            "Epoch: 021/023 | Batch 1024/1256 | Loss: 0.4827\n",
            "Epoch 21/23 \t train_loss=0.4504 \t test_loss=3.3843\t test_accu=0.8656 \t time=90.92s\n",
            "Epoch: 022/023 | Batch 000/1256 | Loss: 0.4590\n",
            "Epoch: 022/023 | Batch 256/1256 | Loss: 0.3642\n",
            "Epoch: 022/023 | Batch 512/1256 | Loss: 0.3591\n",
            "Epoch: 022/023 | Batch 768/1256 | Loss: 0.3840\n",
            "Epoch: 022/023 | Batch 1024/1256 | Loss: 0.5002\n",
            "Epoch 22/23 \t train_loss=0.4469 \t test_loss=5.4193\t test_accu=0.8345 \t time=91.19s\n",
            "Epoch: 023/023 | Batch 000/1256 | Loss: 0.3675\n",
            "Epoch: 023/023 | Batch 256/1256 | Loss: 0.4227\n",
            "Epoch: 023/023 | Batch 512/1256 | Loss: 0.5128\n",
            "Epoch: 023/023 | Batch 768/1256 | Loss: 0.4175\n",
            "Epoch: 023/023 | Batch 1024/1256 | Loss: 0.5226\n",
            "Epoch 23/23 \t train_loss=0.4434 \t test_loss=5.8623\t test_accu=0.8337 \t time=91.35s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "79_VPcLKF0Ki",
        "colab_type": "code",
        "outputId": "a99a2bcf-e8a4-4f82-c180-792706bb03d9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        }
      },
      "source": [
        "plt.plot(mbatch_cost)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f2e64ec1240>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAD8CAYAAAB+UHOxAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAIABJREFUeJzt3Xd8FGX+B/DPNwkBQyeEXkKVoiAQ\nQARBBQRBD3s7vUNFzrOi/vTAgh7eKeqd7ax4YjsV7KKACIj0Fqp0AgQILaGGkpD2/P7Y2WTLzO7s\n7myZ7Of9evFiMzM780x283zn6aKUAhERxZ+EaCeAiIiigwGAiChOMQAQEcUpBgAiojjFAEBEFKcY\nAIiI4hQDABFRnGIAICKKUwwARERxKilaF65fv75KT0+P1uWJiGxp1apVh5VSaVacK2oBID09HZmZ\nmdG6PBGRLYnIbqvOxSogIqI4xQBARBSnGACIiOIUAwARUZxiACAiilMMAEREcYoBgIgoTtkuABSV\nlOGrzL3gUpZERKGJ2kCwYL0xdzvenJeFlOQkDO/SONrJISKyLduVAA6fOgsAyC8sjnJKiIjszXYB\ngIiIrGG7AMCqfyIia9guABARkTVsFwBEop0CIqLKwXYBgFVARETWsF0AcGJBgIgoNLYNAEREFBoG\nACKiOMUAQEQUp2wXABTYCkxEZAXbBQAiIrKG7QKAsP8PEZElbBcAWAVERGQN2wUAJ44IJiIKjW0D\nAEcEExGFxrYBgIiIQmPbAMAqICKi0Ng2ABARUWgYAIiI4hQDABFRnGIAICKKU34DgIhMFpFcEdlg\nsP+PIrJeRH4XkSUi0tX6ZFbISK8HAGhS55xwXoaIqNIzUwL4CMBQH/t3ARiglDofwHMAJlmQLkMN\nalYFAKQkJ4XzMkRElZ7fXFQptUBE0n3sX+Ly4zIAzUJPljFh/08iIktY3QZwF4CZFp+TiIjCwLJ6\nFBG5FI4A0M/HMaMBjAaAFi1aWHVpIiIKgiUlABHpAuC/AEYopY4YHaeUmqSUylBKZaSlpYV4VU4G\nREQUipADgIi0APAtgNuVUttCT5Kf64X7AkREccJvFZCIfAHgEgD1RSQHwDMAqgCAUupdAOMBpAJ4\nW2ugLVFKZYQrwUREZA0zvYBu8bN/FIBRlqWIiIgiwrYjgbkeABFRaGwXADgMgIjIGrYLAEREZA0G\nACKiOGXbAMAmACKi0NguAAhHAhARWcJ2AYCIiKzBAEBEFKdsGwA4DoCIKDS2CwAcB0BEZA3bBQAi\nIrIGAwARUZyybQBQbAQgIgqJ7QIAmwCIiKxhuwBARETWYAAgIopTtg0AbAEgIgqN/QIAGwGIiCxh\nvwBARESWYAAgIopTtg0AHAZARBQa2wUArgdARGQN2wUAIiKyBgMAEVGcsm0AUBwJQEQUEtsFAK4H\nQERkDdsFACIisgYDABFRnLJvAGATABFRSPwGABGZLCK5IrLBYL+IyBsikiUi60Wku/XJdLme9v/R\nM0UY9+3vOFtSGs7LERFVWmZKAB8BGOpj/xUA2mn/RgN4J/Rk+Tdx5hZ8sWIPfli7PxKXIyKqdPwG\nAKXUAgBHfRwyAsAnymEZgDoi0tiqBBqnK9xXICKq3KxoA2gKYK/LzznaNi8iMlpEMkUkMy8vL6iL\nbT10EgCw73hBUO8nIiKHiDYCK6UmKaUylFIZaWlpQZ1j7Z7jFqeKiCg+WREA9gFo7vJzM20bERHF\nMCsCwDQAf9J6A10I4IRS6oAF5yUiojAy0w30CwBLAZwrIjkicpeI3CMi92iHzACwE0AWgPcB3Bu2\n1PqhlMIbc7cj59iZsF7jL59mYnHW4bBdg4goEpL8HaCUusXPfgXgPstS5I+PuYCyj5zBK7O3YeaG\ng5j50MVhuXxhcRlmbTyE+dvysOW5K8JyDSKiSLDdSOCyMuP+n2Va39CzxRwcRkTkj+0CwJkiZu5E\nRFawXQAwg2PEiIj8q1QBgEsF2NeeI2ewarevAedEZDW/jcDkjiuRhUf/l+cBALInDo9ySojih+1K\nALGS/QrLG0Rkc7YLAGYozhRHROSX7QKAr+duicCCwYwtRFRZ2C4AxAouTk9EdscAEKBwFwC2HMzH\nFyv2hPkqRESVNABEopYmXAWAoa8txLhvfw/T2a2xJOsw0sdOx468U9FOChGFoFIFALvWyjzzwwak\nj50e7WSY5lyGc+Uu9tsnsjPbB4DxP2zAnE2Hop2MkHy8dHe0k0BEcch2AcCz8bWwuAyjPskM6ZxF\nJWXYpi01SUQUL2wXAIyMeGtxed25ma6am/bn46tMx1LGz/20CZe/uoDrDBNRXLHdVBBGmfu6vYGt\nFTzsjYUAgBsymiNz9zEAwPEzRWha5xw/13ckIBJjDmIVp8MgqhxsVwIIJOspKCpFfmFxxXuV8jlK\n2EzJwXlI/Gb/FewWA4+eLsK783dwpDiRxnYBIJA8Z8DL89Dl2V/Kfx72xiK0f2qmz3Meyi/EiTPF\nXseYkT52Op6fsTmo99qJXfPPx79eh4kzt2D1nmPRTgpRTLBfADARAZxVFLknz7pt33wgH8WlvnOv\n3s/PRd8Xfw06fZMW7Az6vXZjtwnx8gtLAMDvd4AoXtguAISDZ1A5dbbE1Pv+NWsr/jVrKwBHySGS\nSkrLQnr//uMFSB87HSuC6MvP7JP8OXq6CCuzOU4k1tkuAART/TBmyhocPOE/gzbVBuByzJvzsvDm\nvCwAwG9bcwNPWJDW7DmGtk/OxKLth4M+x7KdRwAgtGkn7FUAoAi6edJS3PDu0mgng/ywXS8gM5Ry\n9O13+n7tfvgq9QfVmGlB5rf90EkMfnVBwO9zPrUv2J6Hfu3qh56QeMGiS8RsO8RpQuzAdiUAM5l1\nQVGpbmOvnj1HzmDDvnwAke/euDgr+Cd4I2VlCmVlzOl8YcGFyMF2AcCMI6eLvLYZdf1zLkXo6ZtV\nOfonj2Leuml/Pl6dvc3nMa2fmIHh/1kU1nTYtRcQEbmrlAEgWItcnsiX7zqCVbuP4tOl2brHuj5F\nlvp44i4oKrXsifzqtxbj9bnbUarlwPO35uHxr9d5Hbf5QL7fc4WSiTtLSnySJrK3uAkAczb7nzDu\npZ+3uv183TtL8fQPG/2+b/QnmboZ6pmiEnQc/zNenLXFdDp9KS5ztGs4u19uPXQSX2YalFRMCiUT\n1xsNrZTC5EW7gh5LQUSRY7sAcF33ZkG9r7A4tG6TAJCbX1j+9OvsUw4Ac7fkeo05AIBT2jHfrt7n\nta+gqBRztwTXcyiWR+Bm7j6GCT9twthv10c7KV44hQWRO9sFgOpVQ++4FEyVzKb9+ej1/Fx8tly/\n2+QrLnXzzuknThQYPwU/9f0GLAygG2dW7smw1L07u4PqKSwuxb9mbUVhcanf85SUluG5nzZhvzah\nnq97j7Z4nseJyJWpACAiQ0Vkq4hkichYnf0tRGSeiKwRkfUiMsz6pFrnzx+uCPg9ztWvlu4wzjCd\nHvpiDQCUd/HMO3nWa+DWnqOnfZ6juLQMe4+eAeDo0jrolcC7i5qx/4T+1BcFRaXo8PTPeHNeFj5Y\ntMvveRZsz8MHi3bhye82hCOZXhZnHcZl//4NZ0v8Byci0uc3AIhIIoC3AFwBoBOAW0Skk8dhTwH4\nUinVDcDNAN62OqFWCuTJ29MiE103d2sZt6svVu4N6Dp//3EjLn5pHo6cOuvVyBzs8+vM3w8gfex0\nr6fzIp1Rxf9bVrFIjeuYCgC6PaG05gmUaC/C3VNo/A8bsDPvdHmQJKLAmSkB9AKQpZTaqZQqAjAF\nwAiPYxSAWtrr2gD2W5dE+9HLoJ/+foPHMb6zcecoX9e2hlC9M38HACD7sO/SB4DynkZ6fM2IakVb\nS7gEE5S+zNyL9LHTcUynazGR3ZkJAE0BuD6+5mjbXD0L4DYRyQEwA8ADlqQuRgx+Zb6pevBw8ayy\ntroK22zj6OfL97hVgUWzKl0vxf9bthvpY6f7ncspkHQ7S0J7WNKgSsiqRuBbAHyklGoGYBiAT0XE\n69wiMlpEMkUkMy8vL6gLnSmKfEa8PfcUsnLND23fkXfasr7/4Zq7PpizPvHd77jl/WWWpyU0Fbn5\nfxc6ZmLNO3kWa/ceR/rY6Vif41goaM+RM9iw/0RUUlgZKKVQEMTfHtdeiG1mAsA+AM1dfm6mbXN1\nF4AvAUAptRRANQBek9QopSYppTKUUhlpaWlBJThSD52bPAZTBfo1fsVgxO7Wgyexavcxvzfiq6dK\nJP6mfF3DzB91cWkZPl++p7z94mxJKZ77aRNOFkamd9BcbdzHb1sdDxr9X55XXj3lL/lKKSzZcTgu\nM6/v1+zDhn3egfKz5XvQcfzPbHOpZMwEgJUA2olIKxFJhqORd5rHMXsADAQAEekIRwAI7hHfjypJ\nkem56pwfyCnQzOC7Nd59/wFgyGsLcN07S0yfx0xB4tOl2aEtcuLnGp+6NAi7co1RnvEqc/cxPPHd\n7+XrLk9duRcfLNqF1+ds93ktrwZnIyEEKH9dVL9ZvQ+3vr8c367eh6NxVvc/ZupaXKkzlcjMDQcA\nALuPMABUJn5zU6VUCYD7AcwCsBmO3j4bRWSCiPxBO+xRAHeLyDoAXwAYqeLx8SkA/koyOcccf2iv\nzN6KB7RupUae/mEjrn3bfFAJVCiZ4EmtEbtEm461xE9Eu//z1X7PqZTCTq0h2z0Iicsxxu+/+5NM\nn+ffc8Rx7pxjBcg55hjXcLrIusZ4f8rKFCYt2BGx0pIZi7P8d38m+zE1qkopNQOOxl3XbeNdXm8C\n0NfapOmLVrtjoOFML/4F0i7gXLVqxu8Hvfa9MFN/aok7P1ppeL72T87U7e4JxNYsyb9s8j9lxwI/\n3Xhd78eq70skH2d+3ZKL52dsQVbuKbx0fdfIXZjiju1GAkcrswr0uvt1FqBxdsMMl199TC1hlPkb\n8dUzKNTP4PCpsyE93Ro1RlaW8b1ntWowsyvTxTKjwLl6zzF8vCQ7omkhb7YLANGy7dDJkM/x8qyK\nyeYi0YVy7uZDeEtbscyXQJPi/KMOdk3gjH/MwYCXfwvg+Nm4/NX5uvsEjtlY08dOL68WAvx3bQ20\nMdN5p1m5JzHhx02WNRAXFpeix3Oz8ctG75JeLLLqe3vt20vwzDT/Ey36c+BEAZ6dttHnjLxWOVNU\nUj4jQGXBAGBSKKOHzTLdAGrSXR9nugUdI55/OmYzRxHgvs9XY+hrCwwzBqOMOJB2hcOninyuMFXs\nUbpxbeQ1Spdeu4pSCqt2H/MZOu74aCUmL96FvUcLfKbZrIMnCnHkdBH+OWOzJefz5/TZErw8awuK\nSspw8EQh9tisUffV2dvw3ZqKGXAf/3o9PlqS7XNOK6vc/UkmBv5b/0HErmy3JGRlKebn5nvPHnrT\npOivofrzhgO453+rkdGyrqnjp68/EOYUBW70J5m4qWdzn8eU6TzB/7B2P8ZMXYsW9VIM31de+onA\nF9E1ic4SR6gT2b3x63a8N38nGtU+p3x0evbE4SGdM5Jen+voRXZNN8eswJF48ndyNoQrpSrNhIIs\nAUTJTp3pGNbsOR6RaysYd5V0dn/1HAcRrJxjBZi/zbhH8IETBeVrHJvl72/v8KmioBptnZ+Jc9Tv\n3C0uDdIG1zxy6iz2Hj0DpRRenb0NO/JO4fYPlmPfcf0SwtaDJ/G10WpzzkvpXGvM1LVoNW6G944A\nndXGQhRbXNo04u9jmLfV/JTo7FhoPdsFAH4FzPE1S+bVby3GY19XzNev93flue3DxRUzgjp3Tfhx\nk990fLJ0N/48eYVhpn3Zv+bjxvfMlXzOlpTisn/9hmdMLNLjJCK6A5vMPL+tz6l4n2d7h/P30/v5\nubj4pXlYl3MCr8/djoH/no+F2w/j9Tn6AwGHvLYA//dVxSpuZr/PP6w1P71Wbn4hbnpvaXk126QF\nO9DuSffgEezfkdXPvXd8WNFzbc6mQ17Vea7W5Xh/jku0qUkOnzprGHStVpnikO0CAJlz32f++9Pr\ncWbUnnX3f3fJ7J1PYnprLxsxKqkXBDDHUs6xAuw8fBoH8yt6WPkrNS3aflh3YFOwPAOZc1xDqFUR\nVmasHyzeheW7jmKqNgPt8zO2lHcrDrXmQgFYu9f6kur8bXkY9Ukm3pjrPVAw59gZ7Dte4HM+roem\nrEXfib9ani49lSj/ZwCorOZsNl+0XuMyitiZkQU6q6e/HkHP/eQIIMsDrO7x59Gv1mGWTg8a5x+p\ncy4gKzw0ZY1ljb9WOn6mCOljp+MbP1VLroKtTvl4STaufmuxz2q9YBw55WgTcw68c9XvxXkRy9x9\nqSTV/m4YAAjvauMTft5wAG//FtxYBbOzpTpH2foy4cdNAU08NtnHgjWnDc6zLucE0sdOx0/rK6pW\n/P19u1bD/O0b9yUvjXqhlJUp3cx2y8F8yzJR5/QMHy/NdmwI4yOqszu0c6S6P5Wx3r4y3RMDAGFd\nzgks3XEE9wZZbQQA9/uZrsLJKEN2NXnxLnywaKfX9k+X6s9JpMfs3+i7JgfneT79LfXI8I2627Z+\nYgbGTF3rtX3oawvx58krLM1MnKfafNCRSes9sQY7dkPvWs9O24hVu60p0Zn5NUQ733X+5vSS8ZW2\nbsRpmw3es10AqEzRN1Jcn3KN3PL+MtMPjnrHBVMHnj52uuE+vTmDPoriyNGTISzMY6YBN5RuhZ5v\nXeCjZBFqBuWazo+WZOO6d6ztuhzLtSzOe9fLgt7RSs6u7VN2YLsAQIG7/3NzT+eVKbaafbI3y7PK\nBwhPY6grvc9j//ECw4eg3z16O3lmpvmFxZiqzc56KIoZ1WaLuhgbOVtS6rUGtxGlFP4zdztyTxr/\nPkZ9vBLfr9kX08EpWLYbCEbxIZBgZFXc8vUQrjdy+eq3FhufK4Tswtc7L9IaQ0WAXS8Mx3drckyP\nID9xpmKE9PsLjdtNzNhtoi0HqPhsnG065yQn4orXFxoe5/tc5j7pc5/6Gb1b1cPUv/TR3f/Z8t2o\nX6MqhnRuhHU5J/Dv2duwbNcRfDbqQt3j52zOxZzNuUhKEL/psNtDFAMABS7GvuTrdfqHB6qopAx5\nJ71HZ0dbaZlCtk5m68xoHp66zmufUzh6rezSBssFGkDOe3aW4178jTq2KM3Ldx3Fwu15uLid98JT\nT35XMQK6tMwRPM10OijvIq33/bdp8YBVQBTTwr0ClfNJ/eGpa/HZ8j2WnvvnDf4nePMXS//z63bD\n+WdiMWAZieSUDU63f7DC5/4TBcUBPbFb1YAeS1gCoICZLYqH4vW52/H63O24tXeLsF8LAKb/bu2c\nRgfzC3HP/1aZPt6ZtRw4UeD2283MNl7prec/5/g5p/4o6Fgzd/Mht9HRTrn5hUhICF+m2/Xvv6B/\ne0cJIaEydvI3gQGAYprdZqt08uzjX1qmcJPOlBffrXYsHVpQXIof1u7DQ1PWomvzOpakYXvuyYjN\nMhqKuz6uWKHN9Sm71/NzQz63UgovzNyC23q3RItU70n+nD2mTOX/HlVAU1bswecr9mDa/f1CTme0\nsAqIYtqirPBPww2Ef5Tn8TNFyNzt/TT/prZew4EThXhoimO8wDqLehd9mWluZLBSCl9l7vWqB88v\nLEbGP+YgMzv4vv6BNop+szoHt3+wPOjredp66CQmLdiJv2ilsVCqoirGATjOMfbb3y1pf4om25UA\nYqz9MS7pLVNpVz4b9iwU7HoSkahuW5R1GI99vd4tMxszZQ2Gd2mCw6fO4vp3A+/rLxL879TKtTe0\nNt7yrrO9DUoVZur3zT0k2CuHsl0AILIjvdHA/kQi8weAU9ogN9dG5e/X7q80c94DwJaDJ3H41Fkc\nPmXQcB7AreoFNrv+plgFRHFtfc4J9PLTmFpZGPVyyjPKFE1yHV/gj5meUUDF5HBugoiHrkHUOSGh\nHs8MvLi0DO/8tsOt+stZSvBMxvEzRdiRZ25cRKyxXQmgV3q9aCeBKpncGO5OaWXXw9mb9DPfEm2q\n6FKPR9usXHPr397w3hL88vAA3X2epRizPaN6/MP6oOyr+t+1sLN85xG8t2Anft2Sa3iMa+eEkS5r\nGjiVlJZh1+HTaNewZtDpjQTblQCqV7VdzCIKWqDVQB/4mBnVqEpngvZkPHvTIbftnlNLGNFbrznc\nVSJ9XphrqnHaNab9uM54TibXQHvTpGVemb/7ORUGv1oxNkNvSpAXf96Cwa8uMD1iOlpsFwCIyJiv\nag67e21OxWIxB04U4pXZ3quutX0iuGUzS8rK/E406Tob6FmD6Teckxiu1MZv6C2aZHbqjkhgACCK\nE2ardEKllPJZ3RKsFSae+PVmkTVjZfYxfOxntllfs4E6Pfql8dQcrcdNR/rY6Wj/1MyIfRb+sD6F\niCxx5NRZHMwvxMb9FbN9hrN7rdVrCExbtx8j+7Yy3G+mWst5787Lztp4ECcLS/BV5l63oLj5QD7a\nNqhhPnFhwgBAFKNmbTzk/6AYMuyNhTiUfxZjBrVz226250+gPBfl0WNlV9qTzrUU/JyysLi0fDDf\ne/N34r353osbbY+REgCrgIjIEofyHb2pXOvqf1p/IKA5kaymVwdvJOdYgam5k/wFlWd+2Oj3HG/M\n3e73mEhgACCikJUZ1L3rTfIWSXfodNE0knvyLK78zyK/x10wYbbP/av3GE/gF2tMBQARGSoiW0Uk\nS0TGGhxzo4hsEpGNIvK5tckkoljW/+V50U5CzIiV6h0z/LYBiEgigLcADAaQA2CliExTSm1yOaYd\ngHEA+iqljolIg3AlmIhiT86xgmgngYJgpgTQC0CWUmqnUqoIwBQAIzyOuRvAW0qpYwCglDIeRUFE\nRDHBTABoCmCvy8852jZX7QG0F5HFIrJMRIbqnUhERotIpohk5uXl6R1CREQRYlUjcBKAdgAuAXAL\ngPdFxGtVC6XUJKVUhlIqIy3Ne61OIiKKHDMBYB+A5i4/N9O2ucoBME0pVayU2gVgGxwBgYiIYpSZ\nALASQDsRaSUiyQBuBjDN45jv4Xj6h4jUh6NKyHv0AxERxQy/AUApVQLgfgCzAGwG8KVSaqOITBCR\nP2iHzQJwREQ2AZgH4DGllP9hekREFDWmpoJQSs0AMMNj23iX1wrAI9o/IiKyAY4EJiKKUwwARERx\nigGAiChOMQAQEcUpWwaA6smJ0U4CEZHt2TIAzHjo4mgngYjI9mwZABrWqhbtJBAR2Z4tAwAREYWO\nAYCIKE4xABARxSkGACKiOMUAQEQUp2wZAJISJNpJICKyPXsGgMQE7Hx+GJ4a3jHaSSEisi1bBgAA\nSEgQDOrYMNrJICKyLdsGAAAQ1gQREQXN1gGgRlVT69kQEZEOWweA1BpVdbfXTakS4ZQQEdmPrQMA\nAGRPHO61beHfLotCSoiI7MX2AcBTemoKq4aIiEyodAFA2DJMRGRK5QsA0U4AEZFNVLoAYNblnTiG\ngIjiW9wGgF6t6kU7CUREUVUpAkAj1xXCtDqgP3RtYnj8/Ze2xYWtU8OcKiKi2FYpAsCPD/TDS9d3\nAWCuDeD/hpyL85rW1u1CSkQULypFAEirWRXdmtdx21ajmrmuoCueGIiqSZXi10BEFJBKl/M5u4E+\nMawjHrisrd/jG9SqhuocN0BEcchUABCRoSKyVUSyRGSsj+OuExElIhnWJTE4Naom4dHLz8UXd1/o\ntn36g/28jnWtNkpKEFzZpXGYU+eua7PaEb0eERFgIgCISCKAtwBcAaATgFtEpJPOcTUBPARgudWJ\nNKO2Nv9Pv7b13bb3aVPR2JtWsyo6N/HObF3Hjj0xrCPevLV7eBJpoEmdcyJ6PSIiwFwJoBeALKXU\nTqVUEYApAEboHPccgBcBFFqYPtMa1KyGBY9d6nORmO/uvcjvef7Up2VI6WjKzJyIbMJMAGgKYK/L\nzznatnIi0h1Ac6XUdAvTFrAWqSlISvS+pU6NawEAmtVNMXhnRRHA+f5P7+pVvu38puaqaEb3b40f\nH/CuYgKAHi3rGr6vQ6Naps5PRGSlkBuBRSQBwCsAHjVx7GgRyRSRzLy8vFAvbdpX9/TB4rHGM4QO\n6tjAa5u4BAUFpfu+q1zGGnw4sif+NrQDkg16FNWomoS14wdj7fjBqH2O+3TVIy9K95V8IqKwMBMA\n9gFo7vJzM22bU00A5wH4TUSyAVwIYJpeQ7BSapJSKkMplZGWlhZ8qgNUvWqSz6qZ564+z2tb4zrV\n3H6+95I2Xsf864Yu5a+7NKuNxASBr/Xq66Qko05KMhY8dmn5tqsvaFLefmGWXsAiIgqUmQCwEkA7\nEWklIskAbgYwzblTKXVCKVVfKZWulEoHsAzAH5RSmWFJcRhU0ak2apNWA2/e2g0AoBTw+NAOyJ44\nHB/8uSKuVU1KRL3qyW7vO6dKolsvIuc5XLlm+Eazl9Y0GMcw8drz8drN3ue8rnsz3eOJiIz4DQBK\nqRIA9wOYBWAzgC+VUhtFZIKI/CHcCYyUvm1T0cWjO2Z6anWv4wZ6LETvrM5xZuQignHDKhqia1Zz\n7E/0VTTw0K9tffxwX1+v7RNGdMbNvVqgRtUkpKe6t2f8+8auqG+wQpovvqbMIKLKzdQIKKXUDAAz\nPLaNNzj2ktCTFXmfjbrQcJ/yaAJYO35weRvBp3f1wpxNh9xKAq7VTX3bpOKOvum4Z4B3FZKR7i3r\nonVajfKfL2qTimeu6oxzG9X0+b6VTw7EoFfm47Eh52Lf8UK8OHMLikrLTF/XCjMevBjD3lgYtvN/\n89c+uO6dpWE7P1E84RDYINRJqcjsm9VNwci+rbyO+fH+fli+6wiSEhPwzFWd/Z5z7fjByC8oQVFp\nGVrVdy95fH63d3BqXi8F2UfOAKgY+yAimPvoJeXHtGtQAyM/XIHGtc/BvuMF+Oc156FtWg3cNGkZ\nAODd23rg6OkiTFu3XzdN13Zrim/XOJp7sicOR/pY3528Fj5+KZrXM+ppVeHvf+iMZ6Zt9Hucnh4t\nOYsrkVUq3VQQVmpQy1GlMqRzo4Dfe36z2hh1cWu/xzlLBnVSktEiNQVtG9Qory7y1XD95i0Vg9X+\nN6q37jH926dh5wvDy9sTujWvi94us6AO7tQQt/RqjhVPDMSrN3X1ev8rN10AAGhSu5rXPk8DOzQo\nz/ydbSBf39MH68Zfjo/u6Il74LP3AAAOcUlEQVRJt/dA9sThyJ44XHesxX//lOF2Ll/+2LuF3/QQ\nkX8sAfjQoGY1rBt/uWGDrBV8VevMerg/CopKdfcF0nPIWYXl2d6slIIkJKBBrWpakFtXvu/Rwe0B\nAJ+P6o22DWrAnwSXNo43b+2ON2+t2HfJue4Zul7D9yCXBXr8tWX885rz8dnyPX7TRBSrBnWMjQWp\nWALwo3ZKFbfMzSqPDm5vOGbAqUbVJKTVDLxh15NzHIMz39VrkE5JTiqfHrtqUgIeGNgOAHBR2/po\nUEu/BPDYkHNDThsA1PUIZvVqJOse18Dld/H6zRfoNpSb5dk76+J29Q2OdMieOBzDzjcuCTY2UUry\nZCawUuV0R9/0aCcBAANA1DwwsB22/eOKiF7T2XB9xXmOjEzvSXzThCFYO/5y3fdfcm6a2ziH+y5t\nizGDHIEi0Aywb1tHVVT2xOFY43G9hwa2wz90xmYMaF8xdmTEBU3R1WMKcAD49t6L8NDAdhjSuaHP\nVd8Sde793dt6oGWqdxuGM+NP81EycbbDuI74HtLZ/SnvvkvdOwL05qp0cat7C+OZASKJAcDGZjx4\nMWaN6e/3OM9eTK/ceAFWPDHQsCRwTnKi7nk+uqMXdr7gKCXclOEYGzhmUHv855ZueGKY8RxMeiaP\n7Il1Hhn/s1d1wvQH+6FalUTcdqF7O0GtakkYf5XXHIReureoi4cHt8d7t2cgRbuPx4aci89G9UZy\nYgJevakr7uzbCoM6NcT/7uqNj+90TPlxQfM6GHpeI3z7V8d8Ua69ut7+Yw8AwNXdmsKI81dc16WD\nwHu3u4+FfGxIB7/pN3JXP++OBoGqLAsgzX7Y/3c+1hn9jUUaA4CNdWpSy2/XUKAic3I+9CYnJRhW\n65iRPXE4Xry+YhT0VV2boFqVwL7QVZMSvdoxRvZtpTtbKwA8OLBd+ZgKVzdmGA+Acwa+To1roW/b\n+tj2zytwTbdmGH9VJ1RJTEC/dvUxoH0afh5zMcYMau83zd1a1EXr+t5jQ8ZeUZGxX96pIS5qk+o1\n8WCzuo4G/Um39yiv/3UNFq3qV/c5DfnTV3Yq7xTgbJOa88gAryBq1jMmgqnVXrzufEvOE8x4F9LH\nABAHlJYTWt+SEV73XtIGf+nv6EnVv73+1CFt0kzUo/u58Q6NapkeqKdXNXXPgDbome4o0rdtWAOf\n330hunkU8ec8MgAAcHnnRnj7j93x4cieuLRDxT3ddmFL0xmbs+TStkEN1E6pgudG+O9m7OmOvq1w\neaeG5b9ff67tblz6ibRa5wQ2dUqs8VyjJJoYAOKIwawTMevxoR0wblhHZE8cjvYN9Us6l2pdRv/S\nvzWWjjOe8M8yBr/DGzOaY8nYy7zqdj8c2ROv3NjVrYSUnJSASzs0QI+W9fDmrd2wecJQ3NWvFZ4a\n3tHU4kD1qie7tYfc3ie9/LVng7qnnx7oVz5l+qQ/ZbiNWvclI8DxF67degFg1wvD3CZYDEViguDD\nO3oa7m9Yyz2Q+urGfF7TWnj3tuDW/9ArDZrhukZJtDEAxIGXru+CXun10KJecF/YWNa+YU1kTxyO\nccM6onHt0NdicDaMJxmUCJobTCkuIroL+1zaoQGu9TFP05VdmpTXByclJuCH+/uVlyacFj7umDyw\nSqL/DLRjY99Ti5/X1Ht8yqYJQ7D66cG6x6enpiB74nA0qeO/ytD5ZPvMVZ2QpKW1ZWoKMp8aBBEx\nXKd7ymj3J+I6WhC7vofj99awVlWvXluXnms8VqRZ3RRsmjAEfVzGvHzz1z547/YeePn6Lm697yaP\n7Imh5zUur6Iza84jwbVDPKj1rosVDABxoEfLevjynj5+u51WNs4M5FyD0oOeuilV8OBlbfH53b0x\nYURnvHtbD7f9zeulYNVTg7Dx70MsTaurqaP7uC1sVLWK43ObPLIn/jKgtc/MKphSXkpyEmoYrIvt\nOYbD087nh5W/7tMmFb8+OsBtevOWqdXLq7aG6gyozJ44HBe2TkX2xOH4SVtLo3pyErY8NxQvXdcF\nW54bioWPX1beA6tXuv+SSHpqdaQkJ+GjO3uienIixg3riB4t62FI50a4IaO522y8zgayV7VBj77c\n0svR8aFDo5po26DiO5UgwJKxl2HGgxf7PcfNPZv7PSaSOBCMKq2rujZxW7PBDBHBI5c7xje4/pG7\nSg1zI2RCguDOvq3w3Zp92Lg/HynJjj/T1mk1MO4K31U2DWo6ntS/+WsfdGteF+/M34GXZ201fe0q\niYLiUlV+jvObOrraplZ33HPNakk4WViibUv2GiPjnMNKbwWNhATBmqcH45XZ2/D0lZ28HkhqVXNO\nrIjyKrNqCYF1LmiZmlLeTlM1KREbJwz1OqZR7Wro17Y+FmUdLp/WpadLYFk2biAemrIGy3cddXvf\n89ecjxeu7QJPsx8ZgCZ1zjG1tGusLf/KAEAUgxISBD/c1xdHzxQZPp27+mxUb+w7XoBh5zfGRW1S\n0b1FXYgI7urXCsWlZfirznoWrpISBM3rnYNHBrfHw1MdI8Jd5106v1ltTB19Ibq3rIuikjJ0fmYW\nJoxwZLS1qiUhXwsKnjwLJHWrJ+uuvwE41uDo2rwO/qYzwLCRVo9vVH8+eWQG7vwoE/+8+nxTXSz1\npk/5/r6+KCwuRaPa1dCnTapXADCaut3OGACIYlRSYkL5E70/fdtWjGS+IaOimqFalURTXVwTEgQL\nH3c0ojsDgCfnPFJVEhPcxhRMf/BirM85YSqdvlRJTDAc3d0ytToW/e1SNNFp5+nTOhWXdWgY8jiH\nC1wGFt4zoA1em7Pd5/GT/tQDkxdno5XLtPGv33wB/vbNevwyZgD6vzwvpPREAgMAUZBmP9wfWw6e\njHYyLFcnpYrX4EFfmtdLMTULbKg81/T+bFRvNKhZFelB9sbxxagTgKu2DWri+WvcxzaMuKApRlzg\n6DL71T19cMO7sT11OQMAUZDaNayJdgE0MNtF5pODQj5HCy0g9Gvre46lUPQN47mTEhPw4R09cceH\nK4M+h2u7wm//d0nE1+YwQ1Qgod5CGRkZKjPTNqtGElGAcvMLkVazqm3rzpVS+O/CXbime9OgRx8v\n2XEYh/ILcU0365ZsFZFVSimvNdeDwRIAEYVFKNONxAIRwd0mR0obuahN+EopVoivjuFERFSOAYCI\nKE4xABARxSkGACKiOMUAQEQUpxgAiIjiFAMAEVGcYgAgIopTURsJLCJ5AHYH+fb6AA5bmJxYwHuK\nfZXtfgDek1243lNLpZT+GqkBiloACIWIZFo1FDpW8J5iX2W7H4D3ZBfhuidWARERxSkGACKiOGXX\nADAp2gkIA95T7Kts9wPwnuwiLPdkyzYAIiIKnV1LAEREFCLbBQARGSoiW0UkS0TGRjs9vohItoj8\nLiJrRSRT21ZPRGaLyHbt/7radhGRN7T7Wi8i3V3O82ft+O0i8ucI38NkEckVkQ0u2yy7BxHpof2O\nsrT3hn31EIN7elZE9mmf1VoRGeayb5yWvq0iMsRlu+53UURaichybftUEUkO8/00F5F5IrJJRDaK\nyEPadtt+Tj7uyc6fUzURWSEi67R7+ruvdIhIVe3nLG1/erD3akgpZZt/ABIB7ADQGkAygHUAOkU7\nXT7Smw2gvse2lwCM1V6PBfCi9noYgJkABMCFAJZr2+sB2Kn9X1d7XTeC99AfQHcAG8JxDwBWaMeK\n9t4ronRPzwL4P51jO2nfs6oAWmnfv0Rf30UAXwK4WXv9LoC/hvl+GgPorr2uCWCblm7bfk4+7snO\nn5MAqKG9rgJgufY71U0HgHsBvKu9vhnA1GDv1eif3UoAvQBkKaV2KqWKAEwBMCLKaQrUCAAfa68/\nBnC1y/ZPlMMyAHVEpDGAIQBmK6WOKqWOAZgNYGikEquUWgDgqMdmS+5B21dLKbVMOb7Zn7icK2wM\n7snICABTlFJnlVK7AGTB8T3U/S5qT8aXAfhae7/r7ycslFIHlFKrtdcnAWwG0BQ2/px83JMRO3xO\nSil1SvuxivZP+UiH6+f3NYCBWroDuldfabJbAGgKYK/Lzznw/aWINgXgFxFZJSKjtW0NlVIHtNcH\nATTUXhvdWyzes1X30FR77bk9Wu7XqkQmO6tLEPg9pQI4rpQq8dgeEVo1QTc4ni4rxefkcU+AjT8n\nEUkUkbUAcuEIsDt8pKM87dr+E1q6Lcsr7BYA7KafUqo7gCsA3Cci/V13ak9Ttu6GVRnuQfMOgDYA\nLgBwAMC/o5ucwIlIDQDfABijlMp33WfXz0nnnmz9OSmlSpVSFwBoBscTe4dopsduAWAfgOYuPzfT\ntsUkpdQ+7f9cAN/B8YEf0orU0P7P1Q43urdYvGer7mGf9tpze8QppQ5pf5xlAN6H47MCAr+nI3BU\nqSR5bA8rEakCR0b5mVLqW22zrT8nvXuy++fkpJQ6DmAegD4+0lGedm1/bS3d1uUV4Wz0sPofgCQ4\nGqZaoaKRo3O002WQ1uoAarq8XgJH3f3LcG+Ye0l7PRzuDXMrtO31AOyCo1Gurva6XoTvJR3uDaaW\n3QO8GxeHRemeGru8fhiOOlYA6Az3BredcDS2GX4XAXwF90a9e8N8LwJHvfxrHttt+zn5uCc7f05p\nAOpor88BsBDAlUbpAHAf3BuBvwz2Xg3TFIk/Not/icPg6BGwA8CT0U6Pj3S21j6AdQA2OtMKRx3e\nXADbAcxx+QMTAG9p9/U7gAyXc90JR0NPFoA7InwfX8BR1C6Go07xLivvAUAGgA3ae96ENjgxCvf0\nqZbm9QCmeWQ0T2rp2wqX3i9G30Xts1+h3etXAKqG+X76wVG9sx7AWu3fMDt/Tj7uyc6fUxcAa7S0\nbwAw3lc6AFTTfs7S9rcO9l6N/nEkMBFRnLJbGwAREVmEAYCIKE4xABARxSkGACKiOMUAQEQUpxgA\niIjiFAMAEVGcYgAgIopT/w8lYqwjdXeIzQAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "SdJ5GlwBYqIq",
        "colab": {}
      },
      "source": [
        "del model"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}